{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2834f7d1-9ca3-42f8-95e9-d15b7199833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec7392-3601-4c0f-a89b-1ef9274fdfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.utils import traitlets as _traitlets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6353da3f-4db4-4568-bbed-a38ea08ac9c9",
   "metadata": {},
   "source": [
    "<h1><center> Learner </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc627321-b834-4f33-84cf-c9428038133e",
   "metadata": {},
   "source": [
    "In this module, we define the model architecture that we will use in our learner. It will be based on [`tsai`](https://github.com/timeseriesAI/tsai) models with a simple tweak: ability to handle categorical features.\n",
    "\n",
    "The strategy to build our model is fairly simple:\n",
    "+ apply a specific 'Embedding` layer to each categorical feature\n",
    "+ concatenate the outcome of the embedding layers with the continuous features\n",
    "+ pass the resulting tensor to a `tsai` existing model\n",
    "The first 2 steps can be handled by the *head* of the network which can be seen as a layer.\n",
    "\n",
    "Once the architecture defined, we can define our learner in the usual way and benefit from `fastai` training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5a01a-5270-47cc-8708-e2c53a4c925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import functools\n",
    "import glob\n",
    "import operator\n",
    "import os\n",
    "import pickle\n",
    "import tempfile\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastai.layers import trunc_normal_\n",
    "from fastai.tabular.all import *\n",
    "from fastai.text.all import *\n",
    "from fastai.vision.all import *\n",
    "from fastcore.basics import *\n",
    "from fastcore.xtras import load_pickle, save_pickle\n",
    "from progressbar import progressbar\n",
    "from tsai.all import *\n",
    "\n",
    "from footSeq.datastruct.core import *\n",
    "from footSeq.datastruct.possessions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef5c8b-fd74-472f-8ecf-0ffb66356265",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = default_device()\n",
    "computer_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bebdf68-2ef7-4a95-9c9c-a2a770d4aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from footSeq.config.mongo import mongo_init\n",
    "\n",
    "mongo_init(\"prod_atlas\")\n",
    "\n",
    "data_path = Path(\"/sequences\")\n",
    "metadata_path = Path(\"./data\")\n",
    "files_list_path = metadata_path / \"file_list.pkl\"\n",
    "\n",
    "if os.path.isfile(files_list_path):\n",
    "    all_files = load_pickle(files_list_path)\n",
    "else:\n",
    "    files_db = L(\n",
    "        Path(os.path.join(data_path, obj[\"file_id\"] + \".csv\"))\n",
    "        for obj in progressbar(PossessionMetadata.objects.only(\"file_id\"))\n",
    "    )\n",
    "    disk_files = data_path.ls(file_exts=\".csv\")\n",
    "    all_files = list(set(files_db) & set(disk_files))\n",
    "    save_pickle(files_list_path, all_files)\n",
    "\n",
    "all_files = L(all_files).shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a0002-976d-46ea-9bba-af90de2359c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_files = 1000000\n",
    "files = all_files[:total_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3fcc6f-ebe3-4760-a342-fa45743746e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_goal_prop = 2\n",
    "valid_pct = 0.3\n",
    "seed = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab3530-fedd-417b-955b-934a0a35dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_goals, test_goals, no_goals = pick_files_from_db(total_n_files=len(files), path=data_path, goal_prop=0.95, no_goal_prop=0.5)\n",
    "n_no_goals_train = int(train_goals.shape[0] * no_goal_prop)\n",
    "files_info = pd.concat([train_goals, no_goals.sample(n_no_goals_train)], axis=0).sample(\n",
    "    frac=1, ignore_index=True\n",
    ")\n",
    "\n",
    "## test files\n",
    "n_no_goals_test = int(test_goals.shape[0] * no_goal_prop)\n",
    "no_goals_left = no_goals[~no_goals.file.isin(files_info.file)].sample(n_no_goals_test)\n",
    "test_files_info = pd.concat([test_goals, no_goals_left], axis=0).sample(\n",
    "    frac=1, ignore_index=True\n",
    ")\n",
    "\n",
    "labels = L(files_info.target.tolist()).unique(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9182b3d6-af07-4c90-967a-c0e9973c4f13",
   "metadata": {},
   "source": [
    "We only want to keep files with at least 2 playing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e705923-1d57-4992-a42e-eed31fdda4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_info = files_info[files_info.nSteps >= 2].reset_index()\n",
    "files_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c411666-154d-49e7-ae99-ce242a14fbe6",
   "metadata": {},
   "source": [
    "In order to test our different examples, let's prepare a batch of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7992ce-db19-4ca2-9e6d-b36c99758f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_names = [\n",
    "    \"standart_name\",\n",
    "    \"possession_name\",\n",
    "    \"attack_status_name\",\n",
    "    \"attack_type_name\",\n",
    "    \"under_pressure\",\n",
    "    \"high_speed\",\n",
    "    \"result_name\",\n",
    "    \"action_subtype_name\",\n",
    "    \"generic_action_type_name\",\n",
    "    \"body_name\",\n",
    "    \"is_poss_team\",\n",
    "    \"is_att_team\",\n",
    "    \"touches\",\n",
    "    \"shot_type\",\n",
    "    \"shot_handling\",\n",
    "]\n",
    "cont_names = [\n",
    "    \"start_x\",\n",
    "    \"start_y\",\n",
    "    \"end_x\",\n",
    "    \"end_y\",\n",
    "    \"time_seconds\",\n",
    "    \"seconds_since_poss\",\n",
    "]\n",
    "\n",
    "files = L(files_info.file.tolist())\n",
    "\n",
    "\n",
    "## splits\n",
    "splits_files = goal_splitter(\n",
    "    files_info_df=files_info, no_goal_prop=no_goal_prop, valid_pct=valid_pct, seed=seed\n",
    ")\n",
    "\n",
    "procs = [FillMissing, Categorify, Normalize]\n",
    "foot_tfm = FootSeqTransform(\n",
    "    files=files,\n",
    "    splits=splits_files,\n",
    "    labels=labels,\n",
    "    procs=procs,\n",
    "    cat_names=cat_names,\n",
    "    cont_names=cont_names,\n",
    ")\n",
    "\n",
    "\n",
    "## to-tensor transform\n",
    "to_tsr = FootSeqToTensor(\n",
    "    cat_names=cat_names,\n",
    "    cont_names=cont_names,\n",
    "    labels=labels,\n",
    "    base_path=data_path,\n",
    "    max_len=10,\n",
    ")\n",
    "\n",
    "## tfmdlist\n",
    "tls = TfmdLists(files, [foot_tfm, to_tsr], splits=splits_files)\n",
    "\n",
    "## params for datalodaers\n",
    "train_seq_lens = L(min(get_sequence_len(file), 10) for file in files[splits_files[0]])\n",
    "valid_seq_lens = L(min(get_sequence_len(file), 10) for file in files[splits_files[1]])\n",
    "\n",
    "\n",
    "## pass the training dataset sequence lengths to SortedDL\n",
    "srtd_dl = partial(SortedDL, res=train_seq_lens)\n",
    "\n",
    "## Pass the validation dataset seq lengths\n",
    "dl_kwargs = [{}, {\"val_res\": valid_seq_lens}]\n",
    "\n",
    "## re-initialise dataloaders\n",
    "srtd_dls = tls.dataloaders(\n",
    "    bs=3, before_batch=pad_seq, dl_type=srtd_dl, dl_kwargs=dl_kwargs\n",
    ")\n",
    "srtd_batch = srtd_dls.one_batch()\n",
    "\n",
    "srtd_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcae71-fb8c-4003-98a3-531bc82c6103",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85718469-c267-44e0-a7ca-f02211bf6ca5",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09788d1-2644-4a24-8ccc-00d349e6f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@delegates()\n",
    "class Embedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    Embedding layer compatible with full pytorch API and truncated normal initialization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ni, nf: int\n",
    "        input and output size of Embedding layer. It is the same\n",
    "        as `num_embeddings` and `embedding_dim` in `torch.nn.Embedding()` module\n",
    "    kwargs: dict\n",
    "        Any argument accepted by `torch.nn.Embedding()` module\n",
    "        a part from `num_embeddings` and `embedding_dim`\n",
    "    std: float\n",
    "        standard deviation applied in the truncated normal\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ni, nf, std=0.01, **kwargs):\n",
    "        kwargs[\"num_embeddings\"], kwargs[\"embedding_dim\"] = ni, nf\n",
    "        super().__init__(**kwargs)\n",
    "        trunc_normal_(self.weight.data, std=std)\n",
    "\n",
    "\n",
    "class MultiEmbedding(Module):\n",
    "    \"\"\"\n",
    "    Muti-dimesnion Embedding layer\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cat_embed: torch.nn.ModuleList\n",
    "        list of Embedding modules in the order in which categorical data appear\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embeds: List[int],\n",
    "        embed_dims: List[int] = None,\n",
    "        embed_p: float = 0.0,\n",
    "        n_cont: int = 0,\n",
    "        std: float = 0.01,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialise the various embedding sizes\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_embdes: List[int]\n",
    "            length of the vocabulary of each categorical feature in the same order as passed in the tensor\n",
    "\n",
    "        embed_dims: List[int]\n",
    "            required size of each categorical feature embedding in the same order as passed in the tensor\n",
    "\n",
    "        embed_p: float\n",
    "            if non zero, applies a dropout layer to the the categorical features after embedding.\n",
    "\n",
    "        n_cont: int, optional\n",
    "            number of continuous features\n",
    "\n",
    "        std: float\n",
    "            standard deviation applied in the truncated normal\n",
    "        kwargs: dict\n",
    "            extra parameters passed to the embedding layer. Should be\n",
    "            compatible with `torch.nn.Embedding()`\n",
    "\n",
    "        \"\"\"\n",
    "        assert n_cont >= 0, \"number of continuous features should be positive\"\n",
    "        self.n_cont = n_cont\n",
    "        ## verify embedding size\n",
    "        if embed_dims is None:\n",
    "            embed_dims = [emb_sz_rule(s) for s in n_embeds]\n",
    "        else:\n",
    "            embed_dims = listify(embed_dims)\n",
    "            if len(embed_dims) == 1:\n",
    "                embed_dims = embed_dims * len(n_embeds)\n",
    "            assert len(embed_dims) == len(n_embeds)\n",
    "\n",
    "        self.emb_drop = nn.Dropout(embed_p)\n",
    "        self.cat_embed = nn.ModuleList(\n",
    "            [\n",
    "                Embedding(ni=n, nf=d, std=std, **kwargs)\n",
    "                for n, d in zip(n_embeds, embed_dims)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_cont=None):\n",
    "        if isinstance(x_cat, tuple):\n",
    "            if len(x_cat) == 2:\n",
    "                x_cat, x_cont = x_cat\n",
    "            elif len(x_cat) == 3:\n",
    "                _, x_cat, x_cont = x_cat\n",
    "            else:\n",
    "                raise ValueError(\"x_cat is a tuple of unknown size\")\n",
    "\n",
    "        x_cat = torch.cat([e(x_cat[..., i]) for i, e in enumerate(self.cat_embed)], -1)\n",
    "        x_cat = self.emb_drop(x_cat)\n",
    "        if self.n_cont != 0:\n",
    "            x_cat = torch.cat([x_cat, x_cont], -1)\n",
    "        return x_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301e8c4-b83b-4839-959c-4dc2dbdb91c4",
   "metadata": {},
   "source": [
    "In order to test this layer, we need to find the vocabulary size of each categorical variable and pass it in `n_embeds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b1925-1aea-446e-968e-734d815630fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embeds = [len(tls.to.classes[n]) for n in tls.to.cat_names]\n",
    "(tls.to.cat_names, n_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd0819-128e-4b94-9ca1-c07e3545b5bc",
   "metadata": {},
   "source": [
    "Now let's initialize the layer and check that it works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d9935a-1a38-4b8e-abfa-e27a47456fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cont = srtd_batch[2].shape[-1]\n",
    "multi_em = MultiEmbedding(n_embeds=n_embeds, n_cont=n_cont).to(device)\n",
    "tsr_em = multi_em(srtd_batch[1], srtd_batch[2])\n",
    "test_eq(\n",
    "    tsr_em.shape[-1],\n",
    "    L(w.weight.shape[-1] for w in multi_em.cat_embed).sum() + n_cont,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d006ceb6-392b-4988-bf30-39fd6f6b5c31",
   "metadata": {},
   "source": [
    "Now let's investigate how we can use the `padding_idx` option. This can be very useful to avoid training useless weight corresponding to padding values. Let's first create a batch with some padded values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6af76-9af5-4b98-8dd4-673cdfff55f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_dls = tls.dataloaders(bs=5, before_batch=pad_seq)\n",
    "padded_batch = reg_dls.one_batch()\n",
    "padded_batch[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e130ffa-0c2b-4483-b02f-83b8540e4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_em_pad = MultiEmbedding(n_embeds, padding_idx=0, n_cont=n_cont).to(device)\n",
    "tsr_em = multi_em_pad(padded_batch[1], padded_batch[2])\n",
    "tsr_em[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2631d51-db1e-45e1-a377-952d32d29491",
   "metadata": {},
   "source": [
    "Notice how the dimension with all zeros (the default padding index) are also filled in with all zeros in the resulting tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176bfb84-0ebe-409e-a02c-f5204684d7a3",
   "metadata": {},
   "source": [
    "## Full Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900283ee-2870-414b-a0d8-b7730cb5316c",
   "metadata": {},
   "source": [
    "Now we are ready to plug in the embedding to any `tsai` learner. Our architecture is fairly straightforward:\n",
    "+ `head` is the head of the network and runs the data through the `multiEmbedding` layer\n",
    "+ `body` takes the output of `head` and run it through the desired architecture selected by the user in `ts_arch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ec3f1-fcc9-4011-9603-9def295ce412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@delegates(build_ts_model)\n",
    "class MixedSeqModel(Module):\n",
    "    \"Sequence model with an embedding head.\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        arch: Module,\n",
    "        n_cont: int,\n",
    "        c_out: int,\n",
    "        embded_config: dict = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Intialise the model architecture\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        arch: Module\n",
    "            one of tsai Model architectures accepted by `build_ts_model()`\n",
    "        c_out: int\n",
    "            number of output layers\n",
    "        n_cont: int\n",
    "            number of continuous features\n",
    "        embed_config: dict\n",
    "            all parameters accepted by the `MultiEmbedding` layer\n",
    "        kwargs:\n",
    "            Extra parameters accepted by `build_ts_model()`\n",
    "\n",
    "        \"\"\"\n",
    "        ## head of the network\n",
    "        embded_config[\"n_cont\"] = n_cont\n",
    "        self.head = MultiEmbedding(**embded_config)\n",
    "\n",
    "        ## inialise the body\n",
    "        self.arch, self.c_out, self.n_cont = arch, c_out, n_cont\n",
    "        self.c_in = L(w.weight.shape[-1] for w in self.head.cat_embed).sum() + n_cont\n",
    "        kwargs[\"arch\"], kwargs[\"c_in\"], kwargs[\"c_out\"] = (\n",
    "            self.arch,\n",
    "            self.c_in,\n",
    "            self.c_out,\n",
    "        )\n",
    "\n",
    "        self.body = build_ts_model(**kwargs)\n",
    "\n",
    "    def forward(self, x_meta, x_cat, x_cont):\n",
    "        if isinstance(x_meta, tuple):\n",
    "            if len(x_meta) == 2:\n",
    "                x_cat, x_cont = x_meta\n",
    "            elif len(x_meta) == 3:\n",
    "                _, x_cat, x_cont = x_meta\n",
    "            else:\n",
    "                raise ValueError(\"x_meta is a tuple of unknown size\")\n",
    "        x = self.head(x_cat, x_cont)\n",
    "\n",
    "        return self.body(x.transpose(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f474eb-7b05-49df-9ce4-cf8420d7eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.models.RNN_FCN import *\n",
    "\n",
    "n_cont = padded_batch[2].shape[-1]\n",
    "\n",
    "## select the architecture\n",
    "ts_model = LSTM_FCN\n",
    "ts_args = {\"bidirectional\": True, \"rnn_layers\": 2, \"shuffle\": False}\n",
    "\n",
    "## LSTM\n",
    "##ts_model = LSTM\n",
    "##ts_args = {\"n_layers\":2, \"bidirectional\":True}\n",
    "\n",
    "model = MixedSeqModel(\n",
    "    arch=ts_model,\n",
    "    n_cont=n_cont,\n",
    "    c_out=2,\n",
    "    embded_config={\"n_embeds\": n_embeds, \"embed_p\": 0.1},\n",
    "    **ts_args\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3808704-6ee1-4351-ba81-7990868a9c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(padded_batch[0], padded_batch[1], padded_batch[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b285822-0482-4dea-90be-0719ed79454d",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401236d6-8454-4c51-98f5-b77d9dff903e",
   "metadata": {},
   "source": [
    "Defining a learner at this stage is straightforward, we just need to decide on the appropriate loss function to use, pass the `dataloaders` and the metrics we want to track. Moreover, the `tsai` `ts_learner` function provides a great interface that we could extend to meet our purposes: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d4070-1a1d-4939-8d8b-1c3235c879bf",
   "metadata": {},
   "source": [
    "## Specific class for the `Learner`\n",
    "We define a specific class `MixedSeqLearner` that knows how to predict the sequence and how to show results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d253132-e6e6-48a1-b91d-cc79fb482ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class MixedSeqLearner(Learner):\n",
    "    \"`Learner` for mixed sequence data\"\n",
    "\n",
    "    def predict(self, files: List[Path]):\n",
    "        \"Predict a sequence of play read from a file\"\n",
    "        with self.no_bar():\n",
    "            dl = self.dls.test_dl(listify(files))\n",
    "            preds, _, cls_preds = self.get_preds(dl=dl, with_decoded=True)\n",
    "            labels = dl.tfms.target_vocab[0]\n",
    "            probs = pd.DataFrame(preds.detach().numpy(), columns=labels)\n",
    "            clss = [labels[i] for i in cls_preds]\n",
    "\n",
    "        return probs, clss\n",
    "\n",
    "\n",
    "@delegates(build_ts_model)\n",
    "def mixed_seq_learner(\n",
    "    arch: Module,\n",
    "    n_cont: int,\n",
    "    c_out: int,\n",
    "    embded_config: dict = None,\n",
    "    # learner args\n",
    "    dls=None,\n",
    "    splitter=trainable_params,\n",
    "    loss_func=None,\n",
    "    opt_func=Adam,\n",
    "    lr=defaults.lr,\n",
    "    cbs=None,\n",
    "    metrics=None,\n",
    "    path=None,\n",
    "    model_dir=\"models\",\n",
    "    wd=None,\n",
    "    wd_bn_bias=False,\n",
    "    train_bn=True,\n",
    "    moms=(0.95, 0.85, 0.95),\n",
    "    # other model args\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Interface to create a `Learner` for sequences with continuous and categorical features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arch: Module\n",
    "        one of tsai Model architectures accepted by `build_ts_model()`\n",
    "    c_out: int\n",
    "        number of output layers\n",
    "    n_cont: int\n",
    "        number of continuous features\n",
    "    embed_config: dict\n",
    "        all parameters accepted by the `MultiEmbedding` layer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fastai.Learner\n",
    "        Learner object with the `MixedSeqModel` model architecture\n",
    "\n",
    "    \"\"\"\n",
    "    if arch is None:\n",
    "        arch = LSTM\n",
    "\n",
    "    model = MixedSeqModel(\n",
    "        arch=arch, n_cont=n_cont, c_out=c_out, embded_config=embded_config, **kwargs\n",
    "    )\n",
    "    try:\n",
    "        model.body[0], model.body[1]\n",
    "        subscriptable = True\n",
    "    except:\n",
    "        subscriptable = False\n",
    "    if subscriptable:\n",
    "        splitter = ts_splitter\n",
    "    if loss_func is None:\n",
    "        if hasattr(dls, \"loss_func\"):\n",
    "            loss_func = dls.loss_func\n",
    "        elif hasattr(dls, \"train_ds\") and hasattr(dls.train_ds, \"loss_func\"):\n",
    "            loss_func = dls.train_ds.loss_func\n",
    "        elif hasattr(dls, \"cat\") and not dls.cat:\n",
    "            loss_func = MSELossFlat()\n",
    "\n",
    "    learn = MixedSeqLearner(\n",
    "        dls=dls,\n",
    "        model=model,\n",
    "        loss_func=loss_func,\n",
    "        opt_func=opt_func,\n",
    "        lr=lr,\n",
    "        cbs=cbs,\n",
    "        metrics=metrics,\n",
    "        path=path,\n",
    "        splitter=splitter,\n",
    "        model_dir=model_dir,\n",
    "        wd=wd,\n",
    "        wd_bn_bias=wd_bn_bias,\n",
    "        train_bn=train_bn,\n",
    "        moms=moms,\n",
    "    )\n",
    "\n",
    "    # keep track of args for loggers\n",
    "    store_attr(\"arch\", self=learn)\n",
    "\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b60fab-f090-42de-aeea-9bcc776770c7",
   "metadata": {},
   "source": [
    "## Useful Learner methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb88d86-4064-4a88-b71c-3b96b257c2ac",
   "metadata": {},
   "source": [
    "### Predict the full possession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7298cd1-4749-4a4d-8ecd-1340520e1adc",
   "metadata": {},
   "source": [
    "We are particularly interested in predicting how the probability of scoring changes as the sequence progresses: In order to do that, we need to sort the sequences in acending order and produce the probabilities progressively: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f5ee4-69f5-441e-bb8d-18dc5217b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@patch\n",
    "def predict_poss(\n",
    "    self: MixedSeqLearner, seq_df: pd.DataFrame, verbose: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"Predict possession outcome probability in a sequentiel way\"\n",
    "\n",
    "    time_seconds = L(seq_df[\"time_seconds\"].sort_values().tolist()).unique()\n",
    "\n",
    "    def _get_probs(time):\n",
    "        _df = seq_df[seq_df[\"time_seconds\"] <= time]\n",
    "        _dir = tempfile.TemporaryDirectory()\n",
    "        _file = _df._id.values[0] + \".csv\"\n",
    "\n",
    "        file_name = Path(_dir.name) / _file\n",
    "        res = None\n",
    "        _df.to_csv(file_name)\n",
    "        with self.no_bar():\n",
    "            _proba = self.predict([Path(file_name)])[0]\n",
    "        res = pd.DataFrame(\n",
    "            {\n",
    "                \"time_seconds\": time,\n",
    "                \"proba_goal\": _proba[\"goal\"],\n",
    "                \"proba_no-goal\": 1.0 - _proba[\"goal\"],\n",
    "            },\n",
    "            index=[0],\n",
    "        )\n",
    "\n",
    "        return res\n",
    "\n",
    "    if verbose:\n",
    "        _time_probs = [_get_probs(_time) for _time in progressbar(time_seconds)]\n",
    "    else:\n",
    "        _time_probs = [_get_probs(_time) for _time in time_seconds]\n",
    "    time_probs = (\n",
    "        pd.concat(_time_probs, axis=0)\n",
    "        .sort_values([\"time_seconds\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return seq_df.merge(time_probs, on=\"time_seconds\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f69b1-7489-41f7-a8b4-89bb6e52bbcb",
   "metadata": {},
   "source": [
    "#### Predict an entire game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37104cc4-a44c-4ae3-80e8-3eeff9e89447",
   "metadata": {},
   "source": [
    "The next step is to be able to predict an entire game. The strategy is the following:\n",
    "+ extract the data from the database\n",
    "+ predict the possessions one by one\n",
    "+ concatenate the data by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aeaa05-0952-43fd-8615-05143279499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@patch\n",
    "def predict_game(\n",
    "    self: MixedSeqLearner,\n",
    "    game_id: int,\n",
    "    match_df: pd.DataFrame = None,\n",
    "    save: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"Predict (all the possessions in) a game\"\n",
    "    if match_df is None:\n",
    "        match_df = Possession.get_all_game_poss(game_id)\n",
    "\n",
    "    _dir = tempfile.TemporaryDirectory()\n",
    "    poss_info = []\n",
    "\n",
    "    def _save_poss_files(poss_nbr, sep=self.dls.tfms.sep):\n",
    "        poss_df = (\n",
    "            match_df[match_df.possessionNumber == poss_nbr]\n",
    "            .sequence.tolist()[0]\n",
    "            .sort_values([\"time_seconds\"])\n",
    "        )\n",
    "        time_seconds = L(poss_df[\"time_seconds\"].sort_values().tolist()).unique()\n",
    "        game_id, poss_number, start_id, end_id, target = poss_df.possession_id[0].split(\n",
    "            sep\n",
    "        )\n",
    "\n",
    "        def _save_one_file(time):\n",
    "            ## extract training data\n",
    "            _df = poss_df[poss_df[\"time_seconds\"] <= time].copy()\n",
    "            ## create file name\n",
    "            _id = sep.join(\n",
    "                [\n",
    "                    str(game_id),\n",
    "                    str(poss_number),\n",
    "                    str(_df.event_id.values[0]),\n",
    "                    str(_df.event_id.values[-1]),\n",
    "                    target,\n",
    "                ]\n",
    "            )\n",
    "            _df[\"target\"] = target\n",
    "            _df[\"_id\"] = _id\n",
    "            _file_name = _id + \".csv\"\n",
    "            _file_path = Path(_dir.name) / _file_name\n",
    "\n",
    "            ## add last time stamp\n",
    "            poss_info.append(_df[_df.time_seconds == time].copy())\n",
    "\n",
    "            ## save to temporary file\n",
    "            _df.to_csv(_file_path, index=False)\n",
    "            return _file_path\n",
    "\n",
    "        return time_seconds.map(_save_one_file)\n",
    "\n",
    "    files = L(\n",
    "        functools.reduce(\n",
    "            operator.iconcat,\n",
    "            L(match_df.possessionNumber.tolist()).map(_save_poss_files),\n",
    "            [],\n",
    "        )\n",
    "    )\n",
    "    poss = pd.concat(poss_info, ignore_index=False).reset_index(drop=True)\n",
    "\n",
    "    ## preidct and adjust probabilities computed\n",
    "    probas = self.predict(files)\n",
    "    probas = probas[0]\n",
    "    probas[\"_id\"] = files.map(lambda x: x.stem)\n",
    "    probas = (\n",
    "        poss.merge(probas, on=\"_id\", how=\"left\")\n",
    "        .rename(columns={\"no-goal\": \"proba_none\", \"goal\": \"proba_goal\"})\n",
    "        .drop([\"_id\"], axis=\"columns\")\n",
    "    )\n",
    "\n",
    "    probas[\"game_id\"] = game_id\n",
    "    ## add minutes and sec\n",
    "    probas[\"minutes\"] = probas.apply(\n",
    "        lambda row: int((row.period_id - 1) * 45 + row.time_seconds // 60), axis=1\n",
    "    )\n",
    "    probas[\"sec\"] = probas[\"time_seconds\"].values % 60\n",
    "    probas = probas.reset_index(drop=True).sort_values(\n",
    "        [\"possession_number\", \"time_seconds\"]\n",
    "    )\n",
    "\n",
    "    if save:\n",
    "        ## remove all existing documents\n",
    "        ActionValues.objects(game_id=game_id).delete()\n",
    "        ## create an actionValue object for each row\n",
    "        lsave = L(ActionValues(**row.to_dict()) for _, row in probas.iterrows())\n",
    "        ActionValues.objects.insert(lsave)\n",
    "\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22cf3aa-63fa-4d89-8725-a9e1bae4cf30",
   "metadata": {},
   "source": [
    "## Save and Load\n",
    "Finally, we provide a function to load a previously saved learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365367d9-f559-497e-b829-661e0462fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@patch\n",
    "def save_all(\n",
    "    self: MixedSeqLearner,\n",
    "    path=\"export\",\n",
    "    dls_fname=\"dls\",\n",
    "    model_fname=\"model\",\n",
    "    learner_fname=\"learner\",\n",
    "    do_save_dls=True,\n",
    "    verbose=False,\n",
    "):\n",
    "    path = Path(path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    if do_save_dls:\n",
    "        self.dls_type = self.dls.__class__.__name__\n",
    "        dls_fnames = []\n",
    "        self.n_loaders = len(self.dls.loaders)\n",
    "        for i, dl in enumerate(self.dls):\n",
    "            dl = dl.new(num_workers=1)\n",
    "            torch.save(dl, path / f\"{dls_fname}_{i}.pth\")\n",
    "            dls_fnames.append(f\"{dls_fname}_{i}.pth\")\n",
    "\n",
    "    # Saves the model along with optimizer\n",
    "    self.model_dir = path\n",
    "    self.save(f\"{model_fname}\", with_opt=True)\n",
    "\n",
    "    # Export learn without the items and the optimizer state for inference\n",
    "    self.export(path / f\"{learner_fname}.pkl\")\n",
    "\n",
    "    pv(f\"Learner saved:\", verbose)\n",
    "    pv(f\"path          = '{path}'\", verbose)\n",
    "    if do_save_dls:\n",
    "        pv(f\"dls_fname     = '{dls_fnames}'\", verbose)\n",
    "    pv(f\"model_fname   = '{model_fname}.pth'\", verbose)\n",
    "    pv(f\"learner_fname = '{learner_fname}.pkl'\", verbose)\n",
    "\n",
    "\n",
    "def load_all(\n",
    "    path=\"export\",\n",
    "    dls_fname=\"dls\",\n",
    "    model_fname=\"model\",\n",
    "    learner_fname=\"learner\",\n",
    "    device=None,\n",
    "    pickle_module=pickle,\n",
    "    do_load_dls=False,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"Load a learner previously saved\"\n",
    "    if isinstance(device, int):\n",
    "        device = torch.device(\"cuda\", device)\n",
    "    elif device is None:\n",
    "        device = default_device()\n",
    "    if device == \"cpu\":\n",
    "        cpu = True\n",
    "    else:\n",
    "        cpu = None\n",
    "\n",
    "    path = Path(path)\n",
    "    learn = load_learner(\n",
    "        path / f\"{learner_fname}.pkl\", cpu=cpu, pickle_module=pickle_module\n",
    "    )\n",
    "    learn.load(f\"{model_fname}\", with_opt=True, device=device)\n",
    "\n",
    "    if do_load_dls:\n",
    "        loaders = []\n",
    "        dls_fnames = []\n",
    "        for i in range(learn.n_loaders):\n",
    "            dl = torch.load(\n",
    "                path / f\"{dls_fname}_{i}.pth\",\n",
    "                map_location=device,\n",
    "                pickle_module=pickle_module,\n",
    "            )\n",
    "            dl = dl.new(num_workers=0)\n",
    "            dl.to(device)\n",
    "            first(dl)\n",
    "            loaders.append(dl)\n",
    "            dls_fnames.append(f\"{dls_fname}_{i}.pth\")\n",
    "        learn.dls = type(learn.dls)(*loaders, path=learn.dls.path, device=device)\n",
    "\n",
    "    pv(f\"Learner loaded:\", verbose)\n",
    "    pv(f\"path          = '{path}'\", verbose)\n",
    "    if do_load_dls:\n",
    "        pv(f\"dls_fname     = '{dls_fnames}'\", verbose)\n",
    "    pv(f\"model_fname   = '{model_fname}.pth'\", verbose)\n",
    "    pv(f\"learner_fname = '{learner_fname}.pkl'\", verbose)\n",
    "\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306694d0-aa51-44bb-b910-f6b37f7fd4fa",
   "metadata": {},
   "source": [
    "## Define and Train a learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71789a1-bde1-4612-8b40-f4614006e686",
   "metadata": {},
   "source": [
    "### From scratch\n",
    "\n",
    "Let's define a learner now with the `LSTM_FCN` architecture and find an appropriate learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2eccdd-1c79-47c0-8013-244c79a8e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.metrics import *\n",
    "\n",
    "n_cont = padded_batch[2].shape[-1]\n",
    "\n",
    "## prepare dataloaders\n",
    "srtd_dls = tls.dataloaders(\n",
    "    bs=64, before_batch=pad_seq, dl_type=srtd_dl, dl_kwargs=dl_kwargs\n",
    ")\n",
    "\n",
    "## select the architecture\n",
    "ts_model = LSTM_FCN\n",
    "ts_args = {\"bidirectional\": True, \"rnn_layers\": 2, \"shuffle\": False}\n",
    "model_name = \"_\".join(\n",
    "    [\n",
    "        ts_model.__name__,\n",
    "        f'bidir-{ts_args.get(\"bidirectional\",False)}',\n",
    "        f'layers-{ts_args.get(\"rnn_layers\", 1)}',\n",
    "        f\"no_goal_prop-{no_goal_prop}\",\n",
    "        \"full_data\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "## create directory if it does not exist\n",
    "if not os.path.exists(Path(\"./models\") / model_name):\n",
    "    os.makedirs(Path(\"./models\") / model_name)\n",
    "\n",
    "learn = mixed_seq_learner(\n",
    "    arch=ts_model,\n",
    "    n_cont=n_cont,\n",
    "    c_out=2,\n",
    "    embded_config={\"n_embeds\": n_embeds, \"embed_p\": 0.1},\n",
    "    dls=srtd_dls,\n",
    "    loss_func=CrossEntropyLossFlat(),\n",
    "    metrics=[accuracy],\n",
    "    path=Path(\".\"),\n",
    "    model_dir=Path(\"./models\") / model_name,\n",
    "    **ts_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c3142-bc4c-4d7d-bb3c-fbe556438955",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ = learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33d7ea-8e95-42cd-835b-fdb7918be510",
   "metadata": {},
   "source": [
    "We can train the learner for a number of cycles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85f41a-d079-4d9f-a186-65a24fb688bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cycles = 10\n",
    "learn.fit_one_cycle(n_cycles, lr_max=lr_[0], cbs=SaveModelCallback(fname=model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93bc18c-df7e-438b-91e2-4e572c3dc02d",
   "metadata": {},
   "source": [
    "Finally, we save the best learner using the `save_all()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d8f6ef-c8a7-443f-bd33-3b3634e234ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_all(\n",
    "    path=Path(\"./models\") / model_name,\n",
    "    dls_fname=\"dls\",\n",
    "    model_fname=\"model\",\n",
    "    learner_fname=\"learner\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ebbdf-db3d-40ea-8903-37a1ff51039d",
   "metadata": {},
   "source": [
    "## Predict Unseen Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a56cd-cd44-4790-931b-338e37b1f42d",
   "metadata": {},
   "source": [
    "### Predict from files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e294520-c778-430f-bca1-5bf15966a901",
   "metadata": {},
   "source": [
    "We can now compute some predictions on some files. We will select `n_ex` sequences ending with a goal and the same number ending in no-goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1513d4-5995-4bab-8e0d-8d7d2373faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ex = 3\n",
    "\n",
    "test_files = (\n",
    "    test_goals.sample(n_ex)[\"file\"].to_list()\n",
    "    + no_goals[~no_goals.file.isin(files_info.file)].sample(n_ex)[\"file\"].to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da56915-c090-4527-8725-65e48f61812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608567fd-8c29-4e31-86d3-7fbea001fc87",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf1167-b378-42de-a3c8-dfdef95a7136",
   "metadata": {},
   "source": [
    "Looking at raw numbers is fine but it is better to visualize the actions on a pitch. In order to do that, we provide a proper `show_results()` method that knows how to display the sequence together with the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a35ee8-500d-4c31-b26f-e0a458158fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@typedispatch\n",
    "def show_results(\n",
    "    x: tuple,\n",
    "    y,\n",
    "    samples,\n",
    "    outs,\n",
    "    ctxs=None,\n",
    "    max_n=6,\n",
    "    nrows=None,\n",
    "    ncols=1,\n",
    "    figsize=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    n_elems = len(samples)\n",
    "    if max_n > n_elems:\n",
    "        max_n = n_elems\n",
    "    if figsize is None:\n",
    "        figsize = (10, 10 * 2.7)\n",
    "    if ctxs is None:\n",
    "        fig, ctxs = get_grid(\n",
    "            max_n, nrows=None, ncols=ncols, figsize=figsize, return_fig=True\n",
    "        )\n",
    "\n",
    "    ## collect learner if available\n",
    "    if \"learner\" in kwargs:\n",
    "        learn = kwargs[\"learner\"]\n",
    "        labels = learn.dls.tfms.target_vocab[0]\n",
    "        with learn.no_bar():\n",
    "            probs, _, pred_cls = learn.get_preds(dl=[x], with_decoded=True)\n",
    "        probs = pd.DataFrame(probs.detach().numpy(), columns=labels)\n",
    "        for i, ctx in enumerate(ctxs):\n",
    "            pred_class = labels[pred_cls[i].item()]\n",
    "            proba = probs[pred_class].values[i]\n",
    "            title = f\"Actual: {samples[i][1]} \\n Prediction: {pred_class} ({proba:.3f})\"\n",
    "            samples[i].show(ctx=ctx, fig=fig, title=title)\n",
    "    else:\n",
    "        for i, ctx in enumerate(ctxs):\n",
    "            title = f'Actual: {samples[i][1]} \\n Prediction: {[\"goal\",\"no_goal\"][y[i].item()]}'\n",
    "            samples[i].show(ctx=ctx, fig=fig, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f556592-1829-4e8e-927f-889db901a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_fig_size = 12\n",
    "\n",
    "test_dl = learn.dls.test_dl(test_files)\n",
    "learn.show_results(\n",
    "    dl=test_dl,\n",
    "    figsize=(base_fig_size, base_fig_size * 2.7),\n",
    "    max_n=6,\n",
    "    ncols=1,\n",
    "    learner=learn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7f6c1-ca24-4f46-9a77-c219bc5f32d9",
   "metadata": {},
   "source": [
    "### Predict full possession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0485702-c0af-4fd5-8109-a29f11f88dc6",
   "metadata": {},
   "source": [
    "We can predict a full possession in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33507906-3f9b-4cd2-9dd3-bc9aced16373",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_file_path = (\n",
    "    files_info[(files_info.target == \"goal\") & (files_info.nSteps == 5)]\n",
    "    .sample(1)[\"file\"]\n",
    "    .values[0]\n",
    ")\n",
    "seq_df = pd.read_csv(goal_file_path)\n",
    "x2 = learn.predict_poss(seq_df)\n",
    "x2[\n",
    "    [\n",
    "        \"type_name\",\n",
    "        \"attack_type_name\",\n",
    "        \"time_seconds\",\n",
    "        \"player_name\",\n",
    "        \"start_x\",\n",
    "        \"start_y\",\n",
    "        \"end_x\",\n",
    "        \"end_y\",\n",
    "        \"is_poss_team\",\n",
    "        \"is_att_team\",\n",
    "        \"proba_goal\",\n",
    "        \"proba_no-goal\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169020af-1203-447e-ae2b-55795e5328f4",
   "metadata": {},
   "source": [
    "### Predict a full game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4206d-746e-4c44-9d5a-2080eefbda6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from footSeq.config.mongo import mongo_init\n",
    "\n",
    "mongo_init(\"prod_atlas\")\n",
    "\n",
    "game_id = 2162755\n",
    "game_probs = learn.predict_game(game_id, save=True)\n",
    "game_probs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09957e-0943-4a02-8e11-5290fbd3c277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
