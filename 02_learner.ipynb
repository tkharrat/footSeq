{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2834f7d1-9ca3-42f8-95e9-d15b7199833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec7392-3601-4c0f-a89b-1ef9274fdfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3321999/2943078347.py:2: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils import traitlets as _traitlets\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "from IPython.utils import traitlets as _traitlets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6353da3f-4db4-4568-bbed-a38ea08ac9c9",
   "metadata": {},
   "source": [
    "<h1><center> Learner </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc627321-b834-4f33-84cf-c9428038133e",
   "metadata": {},
   "source": [
    "In this module, we define the model architecture that we will use in our learner. It will be based on [`tsai`](https://github.com/timeseriesAI/tsai) models with a simple tweak: ability to handle categorical features.\n",
    "\n",
    "The strategy to build our model is fairly simple:\n",
    "+ apply a specific 'Embedding` layer to each categorical feature\n",
    "+ concatenate the outcome of the embedding layers with the continuous features\n",
    "+ pass the resulting tensor to a `tsai` existing model\n",
    "The first 2 steps can be handled by the *head* of the network which can be seen as a layer.\n",
    "\n",
    "Once the architecture defined, we can define our learner in the usual way and benefit from `fastai` training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5a01a-5270-47cc-8708-e2c53a4c925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from random import sample\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import progressbar\n",
    "from fastai.layers import trunc_normal_\n",
    "from fastai.tabular.all import *\n",
    "from fastai.text.all import *\n",
    "from fastcore.basics import *\n",
    "\n",
    "from footSeq.datastruct.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bebdf68-2ef7-4a95-9c9c-a2a770d4aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data\"\n",
    "target_data = pickle.load(open(os.path.join(data_path, \"targets.pkl\"), \"rb\"))\n",
    "df_main = pd.read_pickle(os.path.join(data_path, \"sequences_df.pkl\")).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65dce9f-27bb-4898-955c-7c227ac3bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## available playing ids\n",
    "seq_ids = L(set(df_main[\"_id\"].tolist()) & set(target_data.keys()))\n",
    "labels = L(target_data.values()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db61eea-aeb0-4b9b-9646-f65676f36805",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl2ids = {l: L(k for k, v in target_data.items() if v == l) for l in labels}\n",
    "ids2size = df_main.groupby(\"_id\").size().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c411666-154d-49e7-ae99-ce242a14fbe6",
   "metadata": {},
   "source": [
    "In order to test our different examples, let's prepare a batch of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7992ce-db19-4ca2-9e6d-b36c99758f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 6,  1,  3,  1,  1,  1,  2,  5],\n",
       "          [15,  1,  3,  1,  1,  1,  2,  5],\n",
       "          [ 6,  1,  3,  1,  1,  1,  2,  5],\n",
       "          [15,  1,  3,  1,  1,  1,  2,  5],\n",
       "          [ 6,  1,  3,  1,  1,  1,  2,  5],\n",
       "          [15,  1,  3,  1,  1,  1,  2,  5],\n",
       "          [ 6,  1,  3,  1,  1,  1,  2,  5],\n",
       "          [15,  1,  3,  1,  1,  1,  2,  5],\n",
       "          [ 6,  1,  3,  1,  1,  1,  2,  5],\n",
       "          [15,  1,  3,  1,  1,  1,  2,  5]],\n",
       " \n",
       "         [[15,  1,  6,  1,  1,  1,  2,  5],\n",
       "          [15,  1,  6,  1,  1,  1,  2,  5],\n",
       "          [15,  1,  6,  1,  1,  1,  2,  5],\n",
       "          [ 6,  1,  6,  1,  1,  1,  2,  5],\n",
       "          [15,  3,  6,  1,  1,  1,  2,  5],\n",
       "          [ 6,  1,  6,  1,  2,  1,  2,  5],\n",
       "          [15,  1,  6,  1,  2,  1,  2,  5],\n",
       "          [ 6,  1,  6,  1,  2,  1,  2,  5],\n",
       "          [15,  1,  6,  1,  2,  1,  2,  5],\n",
       "          [15,  1,  6,  1,  1,  1,  2,  5]],\n",
       " \n",
       "         [[15,  1,  7,  1,  1,  1,  2,  5],\n",
       "          [ 6,  1,  7,  1,  1,  1,  2,  5],\n",
       "          [15,  1,  7,  1,  1,  1,  2,  5],\n",
       "          [ 6,  1,  7,  1,  1,  1,  2,  5],\n",
       "          [15,  1,  7,  1,  1,  1,  2,  5],\n",
       "          [ 6,  1,  7,  1,  1,  1,  2,  5],\n",
       "          [15,  1,  7,  1,  1,  1,  2,  5],\n",
       "          [ 6,  1,  7,  1,  1,  1,  2,  5],\n",
       "          [15,  1,  7,  1,  1,  1,  2,  5],\n",
       "          [ 6,  1,  7,  1,  1,  1,  2,  5]]], device='cuda:0'),\n",
       " tensor([[[-0.5608,  0.6991, -0.5657,  0.7094, -1.2093, -0.0855],\n",
       "          [-0.5533,  0.6991, -0.4965,  1.2346, -1.2069,  0.0494],\n",
       "          [-0.4854,  1.2167, -0.4965,  1.2346, -1.2057,  0.1168],\n",
       "          [-0.4854,  1.2167, -0.8078,  1.6296, -1.2057,  0.1168],\n",
       "          [-0.7909,  1.6059, -0.8078,  1.6296, -1.2045,  0.1843],\n",
       "          [-0.7909,  1.6059, -0.3736,  1.3603, -1.2045,  0.1843],\n",
       "          [-0.3647,  1.3405, -0.6041,  1.3603, -1.2021,  0.3192],\n",
       "          [-0.5910,  1.3405, -0.2660,  0.8710, -1.1997,  0.4541],\n",
       "          [-0.2591,  0.8583, -0.1699,  0.6690, -1.1997,  0.4541],\n",
       "          [-0.1649,  0.6593, -0.7886,  0.4087, -1.1972,  0.5890]],\n",
       " \n",
       "         [[-0.0593,  0.0267,  0.0184, -0.0626, -0.6654, -0.8274],\n",
       "          [ 0.0199, -0.1459,  0.3258, -1.5035, -0.6642, -0.7600],\n",
       "          [ 0.2651, -1.4995, -0.1930, -1.6067, -0.6617, -0.6251],\n",
       "          [-0.1875, -1.5836, -0.1930, -1.6067, -0.6605, -0.5576],\n",
       "          [-0.1875, -1.5836, -0.7310, -1.3554, -0.6605, -0.5576],\n",
       "          [-0.7155, -1.3359, -0.7579, -1.3644, -0.6593, -0.4902],\n",
       "          [-0.7419, -1.3447, -0.4351, -1.4586, -0.6593, -0.4902],\n",
       "          [-0.4251, -1.4376, -0.3544, -1.0412, -0.6569, -0.3553],\n",
       "          [-0.3459, -1.0262,  0.0607, -0.3050, -0.6557, -0.2878],\n",
       "          [ 0.0614, -0.3007, -0.9116,  1.1942, -0.6521, -0.0855]],\n",
       " \n",
       "         [[ 0.2236,  0.5885,  0.7908,  1.7912,  1.5383, -0.0180],\n",
       "          [ 0.7780,  1.7652,  0.7831,  1.8092,  1.5395,  0.0494],\n",
       "          [ 0.7704,  1.7829,  1.2750,  1.0955,  1.5407,  0.1168],\n",
       "          [ 1.2532,  1.0795,  1.3173,  1.1763,  1.5431,  0.2517],\n",
       "          [ 1.2946,  1.1591,  0.9676,  0.6780,  1.5443,  0.3192],\n",
       "          [ 0.9515,  0.6681,  0.9292,  0.6735,  1.5467,  0.4541],\n",
       "          [ 0.9137,  0.6637,  0.4219,  0.7723,  1.5479,  0.5215],\n",
       "          [ 0.4159,  0.7610,  0.4373,  0.8037,  1.5491,  0.5890],\n",
       "          [ 0.4310,  0.7920,  0.9100,  1.2077,  1.5515,  0.7239],\n",
       "          [ 0.8949,  1.1901,  0.6679,  1.2840,  1.5540,  0.8588]]],\n",
       "        device='cuda:0'),\n",
       " TensorCategory([1, 1, 1], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_names = [\n",
    "    \"type_name\",\n",
    "    \"bodypart_name\",\n",
    "    \"play_pattern_name\",\n",
    "    \"is_home\",\n",
    "    \"under_pressure\",\n",
    "    \"counterpress\",\n",
    "    \"is_poss_team\",\n",
    "    \"result_name\",\n",
    "]\n",
    "cont_names = [\n",
    "    \"start_x\",\n",
    "    \"start_y\",\n",
    "    \"end_x\",\n",
    "    \"end_y\",\n",
    "    \"time_seconds\",\n",
    "    \"seconds_since_poss\",\n",
    "]\n",
    "\n",
    "## splits\n",
    "splits_ids = RandomSplitter()(seq_ids)\n",
    "\n",
    "## fitst transform inspired from tabular pandas\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "foot_tfm = FootSeqTransform(\n",
    "    sequence_df=df_main,\n",
    "    target_dict=target_data,\n",
    "    ids=seq_ids,\n",
    "    splits=splits_ids,\n",
    "    labels=labels,\n",
    "    procs=procs,\n",
    "    cat_names=cat_names,\n",
    "    cont_names=cont_names,\n",
    ")\n",
    "\n",
    "## to-tensor transform\n",
    "to_tsr = FootSeqToTensor(cat_names, cont_names, max_len=10)\n",
    "\n",
    "## tfmdlist\n",
    "tls = TfmdLists(seq_ids, [foot_tfm, to_tsr], splits=splits_ids)\n",
    "\n",
    "## datalodaers\n",
    "train_seq_lens = L(min(ids2size[_id], 10) for _id in seq_ids[splits_ids[0]])\n",
    "valid_seq_lens = L(min(ids2size[_id], 10) for _id in seq_ids[splits_ids[1]])\n",
    "\n",
    "\n",
    "## pass the training dataset sequence lengths to SortedDL\n",
    "srtd_dl = partial(SortedDL, res=train_seq_lens)\n",
    "\n",
    "## Pass the validation dataset seq lengths\n",
    "dl_kwargs = [{}, {\"val_res\": valid_seq_lens}]\n",
    "\n",
    "## re-initialise dataloaders\n",
    "srtd_dls = tls.dataloaders(\n",
    "    bs=3, before_batch=pad_seq, dl_type=srtd_dl, dl_kwargs=dl_kwargs\n",
    ")\n",
    "\n",
    "##sample a batch\n",
    "srtd_batch = srtd_dls.one_batch()\n",
    "\n",
    "srtd_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcae71-fb8c-4003-98a3-531bc82c6103",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85718469-c267-44e0-a7ca-f02211bf6ca5",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09788d1-2644-4a24-8ccc-00d349e6f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "@delegates()\n",
    "class Embedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    Embedding layer compatible with full pytorch API and truncated normal initialization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ni, nf: int\n",
    "        input and output size of Embedding layer. It is the same\n",
    "        as `num_embeddings` and `embedding_dim` in `torch.nn.Embedding()` module\n",
    "    kwargs: dict\n",
    "        Any argument accepted by `torch.nn.Embedding()` module\n",
    "        a part from `num_embeddings` and `embedding_dim`\n",
    "    std: float\n",
    "        standard deviation applied in the truncated normal\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ni, nf, std=0.01, **kwargs):\n",
    "        kwargs[\"num_embeddings\"], kwargs[\"embedding_dim\"] = ni, nf\n",
    "        super().__init__(**kwargs)\n",
    "        trunc_normal_(self.weight.data.cuda(), std=std)\n",
    "\n",
    "\n",
    "class MultiEmbedding(Module):\n",
    "    \"\"\"\n",
    "    Muti-dimesnion Embedding layer\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    cat_embed: torch.nn.ModuleList\n",
    "        list of Embedding modules in the order in which categorical data appear\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embeds: List[int],\n",
    "        embed_dims: List[int] = None,\n",
    "        n_cont: int =0,\n",
    "        std: float = 0.01,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialise the various embedding sizes\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_embdes: List[int]\n",
    "            length of the vocabulary of each categorical feature in the same order as passed in the tensor\n",
    "\n",
    "        embed_dims: List[int]\n",
    "            required size of each categorical feature embedding in the same order as passed in the tensor\n",
    "            \n",
    "        n_cont: int, optional\n",
    "            number of continuous features\n",
    "\n",
    "        std: float\n",
    "            standard deviation applied in the truncated normal\n",
    "        kwargs: dict\n",
    "            extra parameters passed to the embedding layer. Should be\n",
    "            compatible with `torch.nn.Embedding()`\n",
    "\n",
    "        \"\"\"\n",
    "        assert n_cont >=0, \"number of continuous features should be positive\"\n",
    "        self.n_cont = n_cont\n",
    "        ## verify embedding size\n",
    "        if embed_dims is None:\n",
    "            embed_dims = [emb_sz_rule(s) for s in n_embeds]\n",
    "        else:\n",
    "            embed_dims = listify(embed_dims)\n",
    "            if len(embed_dims) == 1:\n",
    "                embed_dims = embed_dims * len(n_embeds)\n",
    "            assert len(embed_dims) == len(n_embeds)\n",
    "\n",
    "        self.cat_embed = nn.ModuleList(\n",
    "            [\n",
    "                Embedding(ni=n, nf=d, std=std, **kwargs)\n",
    "                for n, d in zip(n_embeds, embed_dims)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_cont=None):\n",
    "        x_cat = torch.cat([e(x_cat[..., i]) for i, e in enumerate(self.cat_embed)], -1)\n",
    "        if self.n_cont != 0:\n",
    "            x_cat = torch.cat([x_cat, x_cont], -1)\n",
    "        return x_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301e8c4-b83b-4839-959c-4dc2dbdb91c4",
   "metadata": {},
   "source": [
    "In order to test this layer, we need to find the vocabulary size of each categorical variable and pass it in `n_embeds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b1925-1aea-446e-968e-734d815630fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#8) ['type_name','bodypart_name','play_pattern_name','is_home','under_pressure','counterpress','is_poss_team','result_name'],\n",
       " [22, 4, 10, 3, 3, 3, 3, 7])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_embeds = [len(tls.to.classes[n]) for n in tls.to.cat_names]\n",
    "(tls.to.cat_names, n_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd0819-128e-4b94-9ca1-c07e3545b5bc",
   "metadata": {},
   "source": [
    "Now let's initialize the layer and check that it works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d9935a-1a38-4b8e-abfa-e27a47456fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cont = srtd_batch[1].shape[-1]\n",
    "multi_em = MultiEmbedding(n_embeds=n_embeds, n_cont=n_cont).cuda()\n",
    "tsr_em = multi_em(srtd_batch[0], srtd_batch[1])\n",
    "test_eq(\n",
    "    tsr_em.shape[-1],\n",
    "    L(w.weight.shape[-1] for w in multi_em.cat_embed).sum() + n_cont,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d006ceb6-392b-4988-bf30-39fd6f6b5c31",
   "metadata": {},
   "source": [
    "Now let's investigate how we can use the `padding_idx` option. This can be very useful to avoid training useless weight corresponding to padding values. Let's first create a batch with some padded values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6af76-9af5-4b98-8dd4-673cdfff55f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  1,  9,  2,  1,  1,  2,  5],\n",
       "        [15,  1,  9,  2,  1,  1,  2,  5],\n",
       "        [ 6,  1,  9,  2,  1,  1,  2,  5],\n",
       "        [15,  1,  9,  2,  1,  1,  2,  5],\n",
       "        [ 6,  1,  9,  2,  1,  1,  2,  5],\n",
       "        [15,  1,  9,  2,  1,  1,  2,  5],\n",
       "        [ 6,  1,  9,  2,  1,  1,  2,  5],\n",
       "        [15,  1,  9,  2,  1,  1,  2,  5],\n",
       "        [ 6,  1,  9,  2,  1,  1,  2,  5],\n",
       "        [15,  1,  9,  2,  1,  1,  2,  5]], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_dls = tls.dataloaders(bs=5, before_batch=pad_seq)\n",
    "padded_batch = reg_dls.one_batch()\n",
    "padded_batch[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e130ffa-0c2b-4483-b02f-83b8540e4bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5199, -1.4598,  1.4809, -0.5296, -1.4100,  2.5280,  0.5804, -0.9284,\n",
       "          0.9980, -0.7382,  0.5828,  0.6865, -0.5352, -0.3123, -1.2477, -1.6916,\n",
       "         -2.0771,  0.9703, -0.6094,  1.4018, -0.1036,  0.5364, -1.7615, -1.5067,\n",
       "         -0.5230,  0.5667, -1.6076, -0.5847, -0.4096, -1.3761,  0.6366,  0.2417,\n",
       "         -0.3364, -1.6510, -0.0459,  0.0501,  1.6502,  0.0491,  1.6745, -0.2641,\n",
       "         -0.3553],\n",
       "        [-0.1981,  1.0633,  0.9373, -1.5942,  0.1827, -0.7031,  0.0292, -0.7047,\n",
       "          0.1495, -0.7382,  0.5828,  0.6865, -0.5352, -0.3123, -1.2477, -1.6916,\n",
       "         -2.0771,  0.9703, -0.6094,  1.4018, -0.1036,  0.5364, -1.7615, -1.5067,\n",
       "         -0.5230,  0.5667, -1.6076, -0.5847, -0.4096, -1.3761,  0.6366,  0.2417,\n",
       "         -0.3364, -1.6510, -0.0459,  0.0501,  1.6502, -0.1199,  1.1134, -0.2628,\n",
       "         -0.2878],\n",
       "        [-0.5199, -1.4598,  1.4809, -0.5296, -1.4100,  2.5280,  0.5804, -0.9284,\n",
       "          0.9980, -0.7382,  0.5828,  0.6865, -0.5352, -0.3123, -1.2477, -1.6916,\n",
       "         -2.0771,  0.9703, -0.6094,  1.4018, -0.1036,  0.5364, -1.7615, -1.5067,\n",
       "         -0.5230,  0.5667, -1.6076, -0.5847, -0.4096, -1.3761,  0.6366,  0.2417,\n",
       "         -0.3364, -1.6510, -0.0459, -0.1158,  1.0972,  0.0338,  0.8306, -0.2616,\n",
       "         -0.2204],\n",
       "        [-0.1981,  1.0633,  0.9373, -1.5942,  0.1827, -0.7031,  0.0292, -0.7047,\n",
       "          0.1495, -0.7382,  0.5828,  0.6865, -0.5352, -0.3123, -1.2477, -1.6916,\n",
       "         -2.0771,  0.9703, -0.6094,  1.4018, -0.1036,  0.5364, -1.7615, -1.5067,\n",
       "         -0.5230,  0.5667, -1.6076, -0.5847, -0.4096, -1.3761,  0.6366,  0.2417,\n",
       "         -0.3364, -1.6510, -0.0459,  0.0350,  0.8185,  0.3796, -1.5843, -0.2592,\n",
       "         -0.0855],\n",
       "        [-0.5199, -1.4598,  1.4809, -0.5296, -1.4100,  2.5280,  0.5804, -0.9284,\n",
       "          0.9980, -0.7382,  0.5828,  0.6865, -0.5352, -0.3123, -1.2477, -1.6916,\n",
       "         -2.0771,  0.9703, -0.6094,  1.4018, -0.1036,  0.5364, -1.7615, -1.5067,\n",
       "         -0.5230,  0.5667, -1.6076, -0.5847, -0.4096, -1.3761,  0.6366,  0.2417,\n",
       "         -0.3364, -1.6510, -0.0459,  0.3744, -1.5615,  0.3412, -0.9020, -0.2556,\n",
       "          0.1168],\n",
       "        [-0.1981,  1.0633,  0.9373, -1.5942,  0.1827, -0.7031,  0.0292, -0.7047,\n",
       "          0.1495, -0.7382,  0.5828,  0.6865, -0.5352, -0.3123, -1.2477, -1.6916,\n",
       "         -2.0771,  0.9703, -0.6094,  1.4018, -0.1036,  0.5364, -1.7615, -1.5067,\n",
       "         -0.5230,  0.5667, -1.6076, -0.5847, -0.4096, -1.3761,  0.6366,  0.2417,\n",
       "         -0.3364, -1.6510, -0.0459,  0.3367, -0.8891,  0.3681, -0.4621, -0.2508,\n",
       "          0.3866],\n",
       "        [-0.5199, -1.4598,  1.4809, -0.5296, -1.4100,  2.5280,  0.5804, -0.9284,\n",
       "          0.9980, -0.7382,  0.5828,  0.6865, -0.5352, -0.3123, -1.2477, -1.6916,\n",
       "         -2.0771,  0.9703, -0.6094,  1.4018, -0.1036,  0.5364, -1.7615, -1.5067,\n",
       "         -0.5230,  0.5667, -1.6076, -0.5847, -0.4096, -1.3761,  0.6366,  0.2417,\n",
       "         -0.3364, -1.6510, -0.0459,  0.3631, -0.4555,  0.3681, -0.4621, -0.2496,\n",
       "          0.4541],\n",
       "        [-0.1981,  1.0633,  0.9373, -1.5942,  0.1827, -0.7031,  0.0292, -0.7047,\n",
       "          0.1495, -0.7382,  0.5828,  0.6865, -0.5352, -0.3123, -1.2477, -1.6916,\n",
       "         -2.0771,  0.9703, -0.6094,  1.4018, -0.1036,  0.5364, -1.7615, -1.5067,\n",
       "         -0.5230,  0.5667, -1.6076, -0.5847, -0.4096, -1.3761,  0.6366,  0.2417,\n",
       "         -0.3364, -1.6510, -0.0459,  0.3631, -0.4555,  0.5872, -0.6955, -0.2496,\n",
       "          0.4541],\n",
       "        [-0.5199, -1.4598,  1.4809, -0.5296, -1.4100,  2.5280,  0.5804, -0.9284,\n",
       "          0.9980, -0.7382,  0.5828,  0.6865, -0.5352, -0.3123, -1.2477, -1.6916,\n",
       "         -2.0771,  0.9703, -0.6094,  1.4018, -0.1036,  0.5364, -1.7615, -1.5067,\n",
       "         -0.5230,  0.5667, -1.6076, -0.5847, -0.4096, -1.3761,  0.6366,  0.2417,\n",
       "         -0.3364, -1.6510, -0.0459,  0.5781, -0.6856,  0.5641, -0.5429, -0.2483,\n",
       "          0.5215],\n",
       "        [-0.1981,  1.0633,  0.9373, -1.5942,  0.1827, -0.7031,  0.0292, -0.7047,\n",
       "          0.1495, -0.7382,  0.5828,  0.6865, -0.5352, -0.3123, -1.2477, -1.6916,\n",
       "         -2.0771,  0.9703, -0.6094,  1.4018, -0.1036,  0.5364, -1.7615, -1.5067,\n",
       "         -0.5230,  0.5667, -1.6076, -0.5847, -0.4096, -1.3761,  0.6366,  0.2417,\n",
       "         -0.3364, -1.6510, -0.0459,  0.5555, -0.5352,  0.2259,  0.4805, -0.2447,\n",
       "          0.7239]], device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_em_pad = MultiEmbedding(n_embeds, padding_idx=0, n_cont=n_cont).cuda()\n",
    "tsr_em = multi_em_pad(padded_batch[0], padded_batch[1])\n",
    "tsr_em[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2631d51-db1e-45e1-a377-952d32d29491",
   "metadata": {},
   "source": [
    "Notice how the dimension with all zeros (the default padding index) are also filled in with all zeros in the resulting tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176bfb84-0ebe-409e-a02c-f5204684d7a3",
   "metadata": {},
   "source": [
    "## Full Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900283ee-2870-414b-a0d8-b7730cb5316c",
   "metadata": {},
   "source": [
    "Now we are ready to plug in the embedding to any `tsai` learner. Our architecture is fairly straightforward:\n",
    "+ `head` is the head of the network and runs the data through the `multiEmbedding` layer\n",
    "+ `body` takes the output of `head` and run it through the desired architecture selected by the user in `ts_arch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ec3f1-fcc9-4011-9603-9def295ce412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@delegates(MultiEmbedding.__init__)\n",
    "class footSeqModel(Module):\n",
    "    \"Sequence model with an embedding head.\"\n",
    "\n",
    "    def __init__(self, ts_arch: Module, n_cont: int, c_out: int, ts_kwargs, **kwargs):\n",
    "        \"\"\"\n",
    "        Intialise the model architecture\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ts_arch: Module\n",
    "            one of tsai Model architectures\n",
    "        ts_kwargs: dict\n",
    "            any of the inputs accepted by the selected architecture a part from `c_out`\n",
    "        c_out: int\n",
    "            number of output layers\n",
    "        n_cont: int\n",
    "            number of continuous features\n",
    "        kwarsgs: dict\n",
    "            all parameters accepted by the `MultiEmbedding` layer\n",
    "\n",
    "        \"\"\"\n",
    "        ## head of the network\n",
    "        kwargs[\"n_cont\"] = n_cont\n",
    "        self.head = MultiEmbedding(**kwargs)\n",
    "        \n",
    "        ## inialise the body\n",
    "        self.c_out, self.n_cont = c_out, n_cont\n",
    "        self.c_in = L(w.weight.shape[-1] for w in self.head.cat_embed).sum() + n_cont\n",
    "        ts_kwargs[\"c_in\"], ts_kwargs[\"c_out\"] = self.c_in, self.c_out\n",
    "        self.body = ts_arch(**ts_kwargs)\n",
    "        \n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        ## run through head first\n",
    "        x = self.head(x_cat, x_cont)\n",
    "        \n",
    "        return self.body(x.transpose(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f474eb-7b05-49df-9ce4-cf8420d7eefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "footSeqModel(\n",
       "  (head): MultiEmbedding(\n",
       "    (cat_embed): ModuleList(\n",
       "      (0): Embedding(22, 9)\n",
       "      (1): Embedding(4, 3)\n",
       "      (2): Embedding(10, 6)\n",
       "      (3): Embedding(3, 3)\n",
       "      (4): Embedding(3, 3)\n",
       "      (5): Embedding(3, 3)\n",
       "      (6): Embedding(3, 3)\n",
       "      (7): Embedding(7, 5)\n",
       "    )\n",
       "  )\n",
       "  (body): LSTM(\n",
       "    (rnn): LSTM(41, 100, num_layers=2, batch_first=True, bidirectional=True)\n",
       "    (fc): Linear(in_features=200, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tsai.models.RNN import *\n",
    "\n",
    "n_cont = padded_batch[1].shape[-1]\n",
    "ts_model = LSTM\n",
    "ts_args = {\"n_layers\":2, \"bidirectional\":True}\n",
    "model = footSeqModel(ts_arch=ts_model, n_cont=n_cont, c_out=2, ts_kwargs=ts_args, n_embeds=n_embeds).cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3808704-6ee1-4351-ba81-7990868a9c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0375,  0.0235],\n",
       "        [-0.0329,  0.0624],\n",
       "        [ 0.0071,  0.0449],\n",
       "        [ 0.0592,  0.0041],\n",
       "        [-0.0337,  0.0609]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(padded_batch[0], padded_batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b285822-0482-4dea-90be-0719ed79454d",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401236d6-8454-4c51-98f5-b77d9dff903e",
   "metadata": {},
   "source": [
    "Defining a learner at this stage is straightforward, we just need to decide on the appropriate loss function to use, pass the `dataloaders` and the metrics we want to track:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2eccdd-1c79-47c0-8013-244c79a8e32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(valley=0.002511886414140463)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAos0lEQVR4nO3deXxU9b3/8ddnJpN9AUJYA4RNNtk0ItalLq3gilsVarVeFTewVXur9tqq9dfN661arftWvXWjXBesa11RaRVQpCDIrgQEEpaQhezf3x8zaIxJSCAnZ5b38/HIg8yZc2beCcm8c86Z8/2acw4REUlcAb8DiIiIv1QEIiIJTkUgIpLgVAQiIglORSAikuBUBCIiCS7J7wDt1b17d1dQUOB3DBGRmLJw4cIS51xec/fFXBEUFBSwYMECv2OIiMQUM/u8pft0aEhEJMGpCEREEpyKQEQkwcXcOQIRkZbU1tZSVFREVVWV31F8k5qaSn5+PqFQqM3bqAhEJG4UFRWRlZVFQUEBZuZ3nE7nnGPr1q0UFRUxcODANm+nQ0MiEjeqqqrIzc1NyBIAMDNyc3PbvUfk6R6BmU0G/gQEgQedc39ocv/PgbMbZRkB5DnntnmZy08NDY6qunpq6x119Q0kBQJkpyV94we3qraeL7ZVUlPXQEZKEhkpQZICAbZX1rCjsobSXbWkJAXJTEkiIyWJYMCorW+gtr4BgJy0EF3Sk8lIDibsL4QkrkT/md+br9+zIjCzIHAX8H2gCJhvZnOcc5/uXsc5dwtwS2T9k4Aro7EEtuysYunGnSzZUMrq4nI2llbxZekuisuqCZqRnBQgOSlAXlYKA7pl0D83na7pIcqq6ti5q5Ydu2r5srSKjTt2sam0irqGb84BkRYK0jsnldzMZL4srWLDjl10xDQRoaDRNT2ZbhnJ5GYm0zsnjUF5GQzqnkl+1zRCwQDBAAQDATJSgmSnhkhJCuzxB6m+wVFT10AwEP7aRWTvZWZmUl5ezrp16zjxxBNZsmRJp2fwco9gArDKObcGwMyeAqYAn7aw/jTgSQ/ztEtDg+OFxRv50+srWVNS8dXyvl3S6NsljQP6d6VHVgoNDmrqGqiuq2fzzmo+/XInr326idp6hxlkpSSRkx6id3YahQO60qdLGtlpIZIiL6I1dQ18WVrFptIqisurOXBAV844MJ+B3TNIDQUpr6qjoqaOunpHt4xkumYkk52aRE1dAxU1dZRV1eEcJAWNUDCAc1C6q4YdlbVsr6xle0UNWytq2FZRzdwVxcxeWNTq150cDNAlPUReVgo9slLISElia3kNW8qqKC6rZldteG9mt5y0EN0zk8nNSCEzNbyHkpmSRM/sFPp1Tad/bjq5GckkBQIEg/bV44eCKhCJAotnwRs3QWkR5OTDMdfDmDP9TtXpvCyCvsD6RreLgIObW9HM0oHJwEyvwry3soQbX1jKoO4ZDMzLYFD3DKrrGthUWsWmnVXgYHCPTPbrmQXA7a+vYOnGnYzonc2vThzJ6L45jOidRVbqns/E1zc4KmvqyEhOIhCIrt3Usqpa1pZUsHFHFfUNjnrnqG9ooLy6np27aimrqmN7ReSFv7yatSUV5GamsF/PLA4d0p3MlCRSkoKkhMIlVlJeTUl59VdlUVFST1lVHVsrqlvdq+mSHqJbRjJZKUmkhIKkhoJkpSaRl5lCXlYKuRnJdEkPkZ0aIjst/G9marhktBciHWLxLHjhJ1C7K3y7dH34NuxTGVxzzTUMGDCAyy67DIAbb7wRM2Pu3Lls376d2tpafvOb3zBlypQWH6O+vp5rr72Wt99+m+rqambMmMHFF1/MOeecwxlnnPHVtmeffTZnnXUWJ5988l7nBW+LoLlXwJZeGk4C3m/psJCZXQRcBNC/f/+9CpMaCjCoewZrSip467MtX/1VmxQweman0uAcz3y84av187umcftZ4zh5bJ92v5gHA9amwvBDVmqIMfldGJPv7fNU19WzYfsu1m/fxfaKGuoawoVTXdfAtooatpbXsLWimorqeqpq6ymtrGH9tkpKyqopq65r9bFTQwFyM1LIzUwmNyOZft3S6d8tnQG5GXTPTCYrNYnMlBBZqUmk6zyJtOSNm74ugd1qd4WX70MRTJ06lSuuuOKrIpg1axavvPIKV155JdnZ2ZSUlDBx4kROPvnkFn82H3roIXJycpg/fz7V1dUceuihHHvssVx44YXcdtttTJkyhdLSUubNm8ejjz6611l387IIioB+jW7nAxtbWHcqrRwWcs7dD9wPUFhYuFdHzwsLulFY0A2AuvoGNu6oIjU5QPeMlK9e6Muqalm5pZyt5TUcsV93UpKCe/NUAqQkBRmUl8mgvMx2b1tVW8/WihpKK2vZWVVLaWRPpbyqlvLqOnZU1obLpKKGzTurWbBue4vlkRSwyMnzEL1zwof1+nRJY2BeBiN6ZTGwewZJOkyVmEpbOEza0vI2Gj9+PFu2bGHjxo0UFxfTtWtXevfuzZVXXsncuXMJBAJs2LCBzZs306tXr2Yf47XXXmPx4sXMnj07HKm0lJUrV3LssccyY8YMtmzZwjPPPMPpp59OUtK+v4x7WQTzgaFmNhDYQPjF/odNVzKzHOC7wI88zPINScEA/XPTv7U8KzXEAf27dlYMaUFqKPjVuZi2cM6xvbKWz7dWsKOylrLqOsqr6r4qkdJd4XMlG0urePOzLRSXVX+1bXIwwNCemYzJ78LY/BzG5HdhaM9MncNIBDn54cNBzS3fR2eccQazZ89m06ZNTJ06lccff5zi4mIWLlxIKBSioKCg1bd4Oue48847mTRp0rfuO+ecc3j88cd56qmnePjhh/c5K3hYBM65OjObCbxK+O2jDzvnlprZJZH7742seirwmnOuooWHEmmVmdEtI/zuqLaoqq1nTXEFn23eyfJNZSzdsJMXF2/kyQ+/AL4uh5G9szmooBtHDsujR3aql1+C+OGY6795jgAglBZevo+mTp3K9OnTKSkp4Z133mHWrFn06NGDUCjEW2+9xeeftzgQKACTJk3innvu4eijjyYUCrFixQr69u1LRkYG5513HhMmTKBXr16MGjVqn7OCx9cROOdeAl5qsuzeJrf/AvzFyxwijaWGgozsk83IPtlfLWtocHy+rZLFRTv4dONOPv1yJ28u38LfIu+y2r9vNkcP78mxI3syqk+2zjvEg93nATx419CoUaMoKyujb9++9O7dm7PPPpuTTjqJwsJCxo0bx/Dhw1vd/sILL2TdunUccMABOOfIy8vjueeeA6Bnz56MGDGCU045ZZ9z7mauI96w3okKCwud5iOQzuCcY/mmMt5cvoW3lm/hoy+20+DCbyGeNKoXpx3Ql/375vgdUxpZtmwZI0aM8DuGpyorKxk9ejQfffQROTnN//w1930ws4XOucLm1tdYQyItMDNG9M5mRO9sZhw1hK3l1byxbAuvLt3EXz/4nIffX8uoPtlMPagfU8b3JTtK3ykm8eP111/n/PPP56qrrmqxBPaG9ghE9kLprlrmLNrAkx+u59Mvd5KZksTZB/fn/MMG0lPnE3yTCHsEbaE9ApFOkJMW4pxDCjjnkAIWF+3ggXfX8sC7a3jk/XV8f1RPCnLT6ZmdSq/sVA4fmkdast6KLNFLRSCyj8bkd+HOaeP5z2P344F31/D6p1t4Zckm6iNjSuVlpfCTo4dw1kH9dVV0J3DOJfTJ/L05yqNDQyIeqG9wbK2o5rNNZdzxxkrmr9tOftc0fnXiSCaNav4iItl3a9euJSsrK2GHot49H0FZWdm35iNo7dCQikDEY8453llRzM2vfMayL3fyX8cPZ/rhgxLyhcprmqGs5RnKdI5AxEdmxpHDejBxUC4/+9sn/O6l5RRt38UNJ40iGGWDEsa6UCjUrpm5JExFINJJUkNB7pw6nvwuadw3dw1LN+6kf7d0GpwjKRDg0iMHM6RH+8dmEtlXKgKRThQIGL84fgT53dJ56N01FJdVEzAoLqvmg7VbmTPzsDYPlSHSUXSOQCQKfLJ+Bz+475+M79eFv154sAa9kw7X2jkC/bSJRIGx/bpw8+mj+WDtNn79wlK/40iC0aEhkShx6vh8lm8q47531jCydw4/PHjvJmESaS/tEYhEkasnDefwod35zYuf8mXprj1vINIBVAQiUSQYMH536mjqGxy/+fsyv+NIglARiESZft3SmXnUEF7895e8u7LY7ziSAFQEIlFo+hGDKMhN54bnl1JdV+93HIlzKgKRKJQaCnLjyaNYU1LBg++u9TuOxDkVgUiUOnJYDyaP6sWdb65k/bZKv+NIHFMRiESx608aSdCMXz63ZK+GFxZpCxWBSBTr0yWN/5w0jHdWFPPC4i/9jiNxSkUgEuXOPaSAsfk53PTCUnZU1vgdR+KQikAkygUDxu9PG8P2ylp+/9Jyv+NIHFIRiMSAkX2yufDwgTy9YD0frt3mdxyJMyoCkRhxxTH70TM7hdtfX+F3FIkzKgKRGJGWHOTCwwYxb/VWPv5iu99xJI6oCERiyLSD+5OTFuLut1f7HUXiiIpAJIZkpiRx3ncK+Menm1mxuczvOBInVAQiMea87xSQnhzkXu0VSAdREYjEmK4ZyUyb0J/nP9mooSekQ6gIRGLQhYcPJGDwwLtr/I4icUBFIBKDeuekcdr4fJ6ev56S8mq/40iMUxGIxKiLvjuImvoGHp23zu8oEuM8LQIzm2xmn5nZKjO7toV1jjSzRWa21Mze8TKPSDwZnJfJpJG9eHTeOsqr6/yOIzHMsyIwsyBwF3AcMBKYZmYjm6zTBbgbONk5Nwr4gVd5ROLRJUcOZmdVHU99+IXfUSSGeblHMAFY5Zxb45yrAZ4CpjRZ54fAM865LwCcc1s8zCMSd8b168LPei3ihDeOxd3YBW7bHxbP8juWxBgvi6AvsL7R7aLIssb2A7qa2dtmttDMzm3ugczsIjNbYGYLios1mbfIVxbP4rKyO+hNMYaD0vXwwk9UBtIuXhaBNbOs6RRLScCBwAnAJOBXZrbftzZy7n7nXKFzrjAvL6/jk4rEqjduIlhf9c1ltbvgjZv8ySMxKcnDxy4C+jW6nQ9sbGadEudcBVBhZnOBsYCGVxRpi9Ki9i0XaYaXewTzgaFmNtDMkoGpwJwm6zwPHG5mSWaWDhwMLPMwk0h8yclv33KRZnhWBM65OmAm8CrhF/dZzrmlZnaJmV0SWWcZ8AqwGPgQeNA5t8SrTCJx55jrIZT2jUX1wbTwcpE28vLQEM65l4CXmiy7t8ntW4BbvMwhErfGnBn+942bcKVFbCKXZ7IvYMbu5SJt4GkRiEgnGHMmjDkTA55/ZzW3vLycw4t2MCa/i9/JJEZoiAmROHL2wf3JTk3iHg1RLe2gIhCJI1mpIc49pIBXlm5i1ZZyv+NIjFARiMSZ8w4tICUpwL3vaK9A2kZFIBJnumemMG1Cf577eIMmrpE2URGIxKGLjhiEGdw3V3sFsmcqApE41DsnjTMO7Mes+UVs3lm15w0koakIROLUpd8dTL1z3D9X01lK61QEInGqf246U8b24YkPvmCrprOUVqgIROLYZUcNpqqunofeW+t3FIliKgKRODakRxbH79+bx/75OTsqa/yOI1FKRSAS5y4/Zgjl1XU8rL0CaYGKQCTODe+VzXH79+KR99dRWlnrdxyJQioCkQTwk2OGUlZdx0Pva69Avk1FIJIARvTOZvKoXjzy3lrtFci3qAhEEsTuvYKHtVcgTagIRBLEyD7ZHDuyJw+/v5bSXdorkK+pCEQSyE+/N5SyqjoefFdXG8vXVAQiCWRUnxxOGN2bh99bq6uN5SsqApEEc+X3h7Krtp77NAaRRKgIRBLMkB5ZnDKuL4/OW8cWjUwqqAhEEtJPvzeU+gbHn99a5XcUiQIqApEENCA3gx8U9uPJD7+gaLtmMUt0KgKRBHX50UMwM25/faXfUcRnKgKRBNWnSxrnThzAMx8VsWJzmd9xxEcqApEENuOoIWQkJ/Hfr3zmdxTxkYpAJIF1zUjm4u8O4vVlm1mwbpvfccQnKgKRBHf+YQPJy0rh5leW45zzO474QEUgkuDSk5P46TFDmb9uO28u3+J3HPGBikBEOOugfgzsnsHNryynrr7B7zjSyVQEIkIoGODqScNYsbmc2QuL/I4jnUxFICIATN6/FwcO6Mof/7GCiuo6v+NIJ1IRiAgAZsZ1J4yguKxaA9IlGE+LwMwmm9lnZrbKzK5t5v4jzazUzBZFPq73Mo+ItO6A/l05cUxv7p+7mk2lGpAuUXhWBGYWBO4CjgNGAtPMbGQzq77rnBsX+bjJqzwi0jbXTB5OQwP88TVdZJYovNwjmACscs6tcc7VAE8BUzx8PhHpAP26pXPeoQXM/qiIJRtK/Y4jncDLIugLrG90uyiyrKlDzOwTM3vZzEY190BmdpGZLTCzBcXFxV5kFZFGZhw1hK7pydz09091kVkC8LIIrJllTX+iPgIGOOfGAncCzzX3QM65+51zhc65wry8vI5NKSLfkpMW4mfH7seHa7fx8pJNfscRj3lZBEVAv0a384GNjVdwzu10zpVHPn8JCJlZdw8ziUgbTT2oP8N7ZfHbF5dRVVvvdxzxkJdFMB8YamYDzSwZmArMabyCmfUyM4t8PiGSZ6uHmUSkjYIB44aTRrFhxy4e0NtJ45pnReCcqwNmAq8Cy4BZzrmlZnaJmV0SWe0MYImZfQLcAUx1OiApEjUOGZzLcfv34u639XbSeGZted01swxgl3Ouwcz2A4YDLzvnar0O2FRhYaFbsGBBZz+tSMJav62SY259h8mjenHHtPF+x5G9ZGYLnXOFzd3X1j2CuUCqmfUF3gD+A/hLx8QTkWjWr1s6l3x3MHM+2cg/V+vIbTxqaxGYc64SOA240zl3KuGLxEQkAVx25GDyu6Zxw5wl1Gp00rjT5iIws0OAs4EXI8uSvIkkItEmNRTk+hNHsmJzOY/OW+d3HOlgbS2CK4BfAM9GTvgOAt7yLJWIRJ3vj+zJkcPyuP31lWzZqRPH8aRNReCce8c5d7Jz7mYzCwAlzrmfeJxNRKKImXHjSaOoqWvgdy8t8zuOdKA2FYGZPWFm2ZF3D30KfGZmP/c2mohEm4LuGVz83UE8t2gj81aX+B1HOkhbDw2NdM7tBE4BXgL6A+d4FUpEoteMo4bQv1s6v3x2CdV1uuI4HrS1CEJmFiJcBM9Hrh/QhV8iCSg1FOSmKaNYU1LBfe/oiuN40NYiuA9YB2QAc81sALDTq1AiEt2OHNaDE8b05s9vrWJtSYXfcWQftfVk8R3Oub7OueNd2OfAUR5nE5Eodv2JI0kJBrj++SUaqjrGtfVkcY6Z3bp7TgAz+yPhvQMRSVA9s1P5z0nDeHdlCc8v2rjnDSRqtfXQ0MNAGXBm5GMn8IhXoUQkNvxo4gDG9evCr19Yytbyar/jyF5qaxEMds7dEJl2co1z7tfAIC+DiUj0CwaMm08fQ3l1Hb95UdcWxKq2FsEuMzts9w0zOxTY5U0kEYklw3plcemRQ3j24w28/dkWv+PIXmhrEVwC3GVm68xsHfBn4GLPUolITJlx1GAG52Vw3bNLqKiu8zuOtFNb3zX0SWRe4THAGOfceOBoT5OJSMxISQpy8+lj2LBjF7e8+pnfcaSd2jVDWWSO4d3XD1zlQR4RiVGFBd047zsF/GXeOj5Yo3kLYsm+TFVpHZZCROLC1ZOH0b9bOlf/32J21Wj4iVixL0WgK0hE5BvSk5O4+fQxfL61UoeIYkirRWBmZWa2s5mPMqBPJ2UUkRhyyOBczj1kAI/MW8v8ddv8jiNt0GoROOeynHPZzXxkOec0Q5mINOuaycPp2yWNn//tEypr9C6iaLcvh4ZERJqVkZLELWeMZd3WSv7w8nK/48geqAhExBOHDM7lgsMG8tg/P2fuimK/40grVAQi4pmfTxrGkB6ZXD17MaWVtX7HkRaoCETEM6mhILedOY6S8mpumLPE7zjSAhWBiHhqdH4Olx89lOcWbeSFTzRcdTRSEYiI52YcNZjx/btw3bP/ZsMOjVcZbVQEIuK5pGCA288aR32D46qnF1HfoOtRo4mKQEQ6xYDcDH49ZX8+WLuN++au9juONKIiEJFOc/oBfTlhTG9ufW0Fi4t2+B1HIlQEItJpzIzfnTKavKwUfvLkx5Rr7oKooCIQkU6Vkx7iT1PH88W2Sq579t84p/MFfvO0CMxsspl9ZmarzOzaVtY7yMzqzewML/OISHSYMLAbV3xvP55ftJHZC4v8jpPwPCsCMwsCdwHHASOBaWY2soX1bgZe9SqLiESfGUcN4ZBBuVz//FJWbSnzO05C83KPYAKwyjm3xjlXAzwFTGlmvcuB/wM067VIAgkGjNunjiMtOcjMJz6mqlYT2fjFyyLoC6xvdLsosuwrZtYXOBW418McIhKlemancuuZY1m+qYwbnl/qd5yE5WURNDeVZdOzQrcD1zjnWv1TwMwuMrMFZraguFijGIrEkyOH9WDmUUN4esF6nS/wiZdFUAT0a3Q7H2g60Egh8JSZrQPOAO42s1OaPpBz7n7nXKFzrjAvL8+juCLilyu+N5SJg7rxy+f+zWebdL6gs3lZBPOBoWY20MySganAnMYrOOcGOucKnHMFwGzgMufccx5mEpEolBQMcMe08WSlhrj08YW6vqCTeVYEzrk6YCbhdwMtA2Y555aa2SVmdolXzysisalHVip3TB3PupIKrp79ia4v6EQWa9/swsJCt2DBAr9jiIhH7ntnNb9/eTn/dfxwLjpisN9x4oaZLXTOFTZ3n64sFpGoctERgzh+dC/+8PJy5q0q8TtOQlARiEhUMTP++4yxDMrLZOaTH7NR8xd4TkUgIlEnMyWJe390IDV1DVzy14W62MxjKgIRiUpDemRy65ljWVxUyi+e0eB0XlIRiEjUOnZUL676/n48+/EGHnpvrd9x4paKQESi2syjhnDc/r343UvLmLtCIwt4QUUgIlEtEDD+5wdj2a9nFjOf+Ig1xeV+R4o7KgIRiXoZKUk8cG4hScEAFzy6gNLKWr8jxRUVgYjEhH7d0rnvnAPZsH0Xlz6+kNr6Br8jxQ0VgYjEjIMKuvH700Yzb/VWbpizVO8k6iBJfgcQEWmP0w/MZ1VxOfe8vZpB3TO48PBBfkeKeSoCEYk5Pz92GOtKKvjtS8vI75rO5P17+R0ppunQkIjEnEDAuO2scYzN78IVT3/MovU7/I4U01QEIhKTUkNBHvxxIXlZKVz46HzWb6v0O1LMUhGISMzqnpnCI+dNoKaugR8/8iHbK2r8jhSTVAQiEtOG9MjkwR8fRNH2XVzw6HwNULcXVAQiEvMmDOzGn84ax8frd3D5kx9Tp2sM2kVFICJx4bjRvbnxpFH849PNXK9rDNpFbx8Vkbjx4+8UsGlnFfe8vZruGclcdewwvyPFBBWBiMSVqycNY2t5NXe8uYquGcn8x6ED/Y4U9VQEIhJXzIzfnTqaHZW1/PqFT+manswp4/v6HSuq6RyBiMSdpGCAO6aNZ+Kgbvzsb5/wxrLNfkeKaioCEYlLqaEgD5xbyMje2Vz6+Ee8v6rE70hRS0UgInErKzXEY+dPYGBuBtMfW8DCz7f7HSkqqQhEJK51zUjmfy+cQI+sFM575EOWbCj1O1LUURGISNzrkZXK49Mnkp0a4pyHPmD5pp1+R4oqKgIRSQh9u6Tx+IUHk5wU4OwHPmDl5jK/I0UNFYGIJIyC7hk8OX0igYAx7YEPWF1c7nekqKAiEJGEMigvkyenHww4pt3/L9aoDFQEIpJ4hvTI4vELJ1Lf4Jh6/79YtSWxy0BFICIJaVivLJ68aCINzjHtgX+xakvinjNQEYhIwtqvZxZPTp+IczD1/g9YkaAnkFUEIpLQhvbM4qmLJhIwOOu+fybkdQaeFoGZTTazz8xslZld28z9U8xssZktMrMFZnaYl3lERJozpEcmsy4+hPTkJKY98C8++iKxrkD2rAjMLAjcBRwHjASmmdnIJqu9AYx1zo0Dzgce9CqPiEhrCrpn8PTFE+mWkcw5D37AP1dv9TtSp/Fyj2ACsMo5t8Y5VwM8BUxpvIJzrtx9PY1QBqAphUTEN/ld05l18SH07pLGeY98yJvLE2PUUi+LoC+wvtHtosiybzCzU81sOfAi4b2CbzGziyKHjhYUFxd7ElZEBKBndiqzLj6E/XpmcdFjC3l+0Qa/I3nOyyKwZpZ96y9+59yzzrnhwCnA/2vugZxz9zvnCp1zhXl5eR2bUkSkiW4ZyTwx/WAOHNCVK55exP/+c53fkTzlZREUAf0a3c4HNra0snNuLjDYzLp7mElEpE2yUkM8ev4Ejhneg189v5TbX1/B10ey44uXRTAfGGpmA80sGZgKzGm8gpkNMTOLfH4AkAwkzhkaEYlqqaEg9/zoQM44MJ/bX1/Jdc8tob4h/srAszmLnXN1ZjYTeBUIAg8755aa2SWR++8FTgfONbNaYBdwlovXyhWRmBQKBrjljDHkZaVwz9ur2VZew+1Tx5EaCvodrcNYrL3uFhYWugULFvgdQ0QS0EPvreX//f1TDiroygPnFtIlPdnvSG1mZgudc4XN3acri0VE2uiCwwby5x+O55P1pZx2zzzWb6v0O1KHUBGIiLTDiWP68NcLD2ZreQ2n3j2PxUU7/I60z1QEIiLtNGFgN/7v0kNIDQU4675/8erSTX5H2icqAhGRvTCkRxbPXnYow3plcclfF/LA3DUx+/ZSFYGIyF7Ky0rhqYsmcvz+vfntS8v4r2eXUFvf4HesdvPs7aMiIokgNRTkzmnjGZCbzt1vr2ZdSQV3n30AXTNi5x1F2iMQEdlHgYBx9eTh3HrmWBZ+vp1T7n4/pmY8UxGIiHSQ0w7I58mLJlJRXc+pd83jreVb/I7UJioCEZEOdOCArsyZeSj9c9M5/9H53PXWqqg/iawiEBHpYH26pDH7ku9w8tg+3PLqZ8x44iMqquv8jtUiFYGIiAfSkoPcftY4rjt+BK8s2cRpd89jbUmF37GapSIQEfGImTH9iEE8dv7BbCmr4uQ73+Mfn0bfrGcqAhERjx02tDsvXH4YA/MymP7YAm55dXlUDWetIhAR6QS750OeelA/7nprNT968AO2lFX5HQtQEYiIdJrUUJA/nD6G/z5jDB+v384Jd7zHv9b4PxeXikBEpJOdWdiP52YcSlZKEj984F/8+c2VNPh4qEhFICLig+G9snl+5qEcP7o3//PaCs59+EPfDhWpCEREfJKVGuLOaeP5w2mjmb9uG8f/6T3eXVnc6TlUBCIiPjIzpk7oz5yZh9E1PcQ5D33I719aRk1d541iqiIQEYkCw3plMWfmYZx9cH/um7uG0++Zx5ri8k55bhWBiEiUSEsO8ttTR3PfOQeyfnslJ975Hk9++IXnYxWpCEREosykUb145adHML5/F37xzL+Z/tgCSsqrPXs+FYGISBTqlZPK/55/ML86cSRzV5Yw6ba5vLHMm+EpVAQiIlEqEDAuOGwgL8w8jB7ZqXyxrdKT59FUlSIiUW5Yryyen3EoSQHz5PFVBCIiMSA5ybsDODo0JCKS4FQEIiIJTkUgIpLgVAQiIglORSAikuBUBCIiCU5FICKS4MzrwYw6mpkVA59HbuYApa183nRZCChp51M2foy23td0eVtz7v63eyfkbC1jLOZsLW97c7aWcW9ytvX/Pxpz6neo5fti7XdogHMur9mtnHMx+wHc39rnTZcBC/blOdp6X9Plbc3Z6F/Pc7aWMRZz7iFvu3K2lnFvcrbj/z/qcup3KP5+h5r7iPVDQy/s4fOW7t/b52jrfU2XtzXn3mbc07bN3ddaxqa3YyHnnv7/22NP27U3Z3t+TtujM3Lqd6jl+2L1d+hbYu7Q0L4wswXOuUK/c+yJcnYs5ew4sZARlLO9Yn2PoL3u9ztAGylnx1LOjhMLGUE52yWh9ghEROTbEm2PQEREmlARiIgkOBWBiEiCUxFEmNnhZnavmT1oZvP8ztMSMwuY2W/N7E4z+7HfeVpiZkea2buR7+mRfudpiZllmNlCMzvR7ywtMbMRke/jbDO71O88LTGzU8zsATN73syO9TtPS8xskJk9ZGaz/c7SVOTn8dHI9/HsznreuCgCM3vYzLaY2ZImyyeb2WdmtsrMrm3tMZxz7zrnLgH+DjwarTmBKUBfoBYoiuKcDigHUr3I2UEZAa4BZnV0vkZ5OuJnc1nkZ/NMwJO3GnZQzuecc9OB84CzojjnGufcBV7ka047M58GzI58H0/urIztuqItWj+AI4ADgCWNlgWB1cAgIBn4BBgJjCb8Yt/4o0ej7WYB2dGaE7gWuDiy7ewozhmIbNcTeDxKM34PmEr4hevEaP1eRrY5GZgH/DCac0a2+yNwQAzk9OT3Zx8z/wIYF1nnic7I55yLjzmLnXNzzaygyeIJwCrn3BoAM3sKmOKc+z3Q7GEAM+sPlDrndkZrTjMrAmoiN+ujNWcj24GUaMxoZkcBGYR/AXeZ2UvOuYZoyxl5nDnAHDN7EXiiIzN2VE4zM+APwMvOuY86OmNH5exs7clMeO85H1hEJx6xiYsiaEFfYH2j20XAwXvY5gLgEc8SNa+9OZ8B7jSzw4G5XgZrol05zew0YBLQBfizp8m+1q6MzrnrAMzsPKCko0ugFe39Xh5J+JBBCvCSl8GaaO/P5uWE97JyzGyIc+5eL8M10t7vZy7wW2C8mf0iUhidraXMdwB/NrMT2LdhKNolnovAmlnW6tVzzrkbPMrSmnbldM5VEi6sztbenM8QLq3O1O7/cwDn3F86Pkqr2vu9fBt426swrWhvzjsIv5B1tvbm3Apc4l2cNmk2s3OuAviPzg4TFyeLW1AE9Gt0Ox/Y6FOW1ihnx4mFjKCcHS1WcjYWVZnjuQjmA0PNbKCZJRM+KTjH50zNUc6OEwsZQTk7WqzkbCy6MnfWWWmPz8o/CXzJ12+pvCCy/HhgBeGz89cpZ/zkjIWMypm4OWMtswadExFJcPF8aEhERNpARSAikuBUBCIiCU5FICKS4FQEIiIJTkUgIpLgVAQSF8ysvJOfr0PmrLDwvA2lZvaxmS03s/9pwzanmNnIjnh+EVARiDTLzFodh8s5950OfLp3nXPjgfHAiWZ26B7WP4XwiKkiHSKeB52TBGdmg4G7gDygEpjunFtuZicBvyQ8DvxW4Gzn3GYzuxHoAxQAJWa2AuhPeMz4/sDtLjywGmZW7pzLjIwMeiNQAuwPLAR+5JxzZnY8cGvkvo+AQc65FodFds7tMrNFhEemxMymAxdFcq4CzgHGEZ6b4Ltm9kvg9Mjm3/o69/b7JolHewQSz+4HLnfOHQj8J3B3ZPl7wMTIX+FPAVc32uZAwmPZ/zByezjh4bQnADeYWaiZ5xkPXEH4r/RBwKFmlgrcBxznnDuM8It0q8ysKzCUr4cXf8Y5d5BzbiywjPDQBPMIj0nzc+fcOOfc6la+TpE20R6BxCUzywS+A/wtPF8K8PUEOfnA02bWm/Bf22sbbTrHOber0e0XnXPVQLWZbSE841rTqTc/dM4VRZ53EeE9inJgjXNu92M/Sfiv++YcbmaLgWHAH5xzmyLL9zez3xCe0yETeLWdX6dIm6gIJF4FgB3OuXHN3HcncKtzbk6jQzu7VTRZt7rR5/U0/zvT3DrNjTffknedcyea2X7Ae2b2rHNuEfAX4BTn3CeRyXOObGbb1r5OkTbRoSGJSy483ehaM/sBhKdRNLOxkbtzgA2Rz3/sUYTlwKBGUxTucTJ359wK4PfANZFFWcCXkcNRZzdatSxy356+TpE2URFIvEg3s6JGH1cRfvG8wMw+AZYSnhMWwnsAfzOzdwmfyO1wkcNLlwGvmNl7wGagtA2b3gscYWYDgV8BHwD/IFwsuz0F/DzyltPBtPx1irSJhqEW8YiZZTrnyiOTut8FrHTO3eZ3LpGmtEcg4p3pkZPHSwkfjrrP3zgizdMegYhIgtMegYhIglMRiIgkOBWBiEiCUxGIiCQ4FYGISIJTEYiIJLj/D+ZRxPsVViHxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastai.metrics import *\n",
    "learn = Learner(srtd_dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy])\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85f41a-d079-4d9f-a186-65a24fb688bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/2 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2167' class='' max='91592' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      2.37% [2167/91592 00:27<18:57 0.0459]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e9d72f-bdb9-4aa0-b4cc-072743ce668f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
