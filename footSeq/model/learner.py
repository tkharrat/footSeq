# AUTOGENERATED! DO NOT EDIT! File to edit: 02_learner.ipynb (unless otherwise specified).

__all__ = ['Embedding', 'MultiEmbedding', 'footSeqModel']

# Cell
import os
import pickle
import warnings
from random import sample
from typing import List, Tuple

import numpy as np
import pandas as pd
import progressbar
from fastai.layers import trunc_normal_
from fastai.tabular.all import *
from fastai.text.all import *
from fastcore.basics import *

from ..datastruct.core import *

# Cell


@delegates()
class Embedding(nn.Embedding):
    """
    Embedding layer compatible with full pytorch API and truncated normal initialization

    Parameters
    ----------
    ni, nf: int
        input and output size of Embedding layer. It is the same
        as `num_embeddings` and `embedding_dim` in `torch.nn.Embedding()` module
    kwargs: dict
        Any argument accepted by `torch.nn.Embedding()` module
        a part from `num_embeddings` and `embedding_dim`
    std: float
        standard deviation applied in the truncated normal

    """

    def __init__(self, ni, nf, std=0.01, **kwargs):
        kwargs["num_embeddings"], kwargs["embedding_dim"] = ni, nf
        super().__init__(**kwargs)
        trunc_normal_(self.weight.data.cuda(), std=std)


class MultiEmbedding(Module):
    """
    Muti-dimesnion Embedding layer

    Attributes
    ----------
    cat_embed: torch.nn.ModuleList
        list of Embedding modules in the order in which categorical data appear

    """

    def __init__(
        self,
        n_embeds: List[int],
        embed_dims: List[int] = None,
        n_cont: int =0,
        std: float = 0.01,
        **kwargs
    ):
        """
        Initialise the various embedding sizes

        Parameters
        ----------
        n_embdes: List[int]
            length of the vocabulary of each categorical feature in the same order as passed in the tensor

        embed_dims: List[int]
            required size of each categorical feature embedding in the same order as passed in the tensor

        n_cont: int, optional
            number of continuous features

        std: float
            standard deviation applied in the truncated normal
        kwargs: dict
            extra parameters passed to the embedding layer. Should be
            compatible with `torch.nn.Embedding()`

        """
        assert n_cont >=0, "number of continuous features should be positive"
        self.n_cont = n_cont
        ## verify embedding size
        if embed_dims is None:
            embed_dims = [emb_sz_rule(s) for s in n_embeds]
        else:
            embed_dims = listify(embed_dims)
            if len(embed_dims) == 1:
                embed_dims = embed_dims * len(n_embeds)
            assert len(embed_dims) == len(n_embeds)

        self.cat_embed = nn.ModuleList(
            [
                Embedding(ni=n, nf=d, std=std, **kwargs)
                for n, d in zip(n_embeds, embed_dims)
            ]
        )

    def forward(self, x_cat, x_cont=None):
        x_cat = torch.cat([e(x_cat[..., i]) for i, e in enumerate(self.cat_embed)], -1)
        if self.n_cont != 0:
            x_cat = torch.cat([x_cat, x_cont], -1)
        return x_cat

# Cell

@delegates(MultiEmbedding.__init__)
class footSeqModel(Module):
    "Sequence model with an embedding head."

    def __init__(self, ts_arch: Module, n_cont: int, c_out: int, ts_kwargs, **kwargs):
        """
        Intialise the model architecture

        Parameters
        ----------
        ts_arch: Module
            one of tsai Model architectures
        ts_kwargs: dict
            any of the inputs accepted by the selected architecture a part from `c_out`
        c_out: int
            number of output layers
        n_cont: int
            number of continuous features
        kwarsgs: dict
            all parameters accepted by the `MultiEmbedding` layer

        """
        ## head of the network
        kwargs["n_cont"] = n_cont
        self.head = MultiEmbedding(**kwargs)

        ## inialise the body
        self.c_out, self.n_cont = c_out, n_cont
        self.c_in = L(w.weight.shape[-1] for w in self.head.cat_embed).sum() + n_cont
        ts_kwargs["c_in"], ts_kwargs["c_out"] = self.c_in, self.c_out
        self.body = ts_arch(**ts_kwargs)


    def forward(self, x_cat, x_cont):
        ## run through head first
        x = self.head(x_cat, x_cont)

        return self.body(x.transpose(2, 1))