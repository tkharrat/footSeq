# AUTOGENERATED! DO NOT EDIT! File to edit: 02_learner.ipynb (unless otherwise specified).

__all__ = ['Embedding', 'MultiEmbedding', 'MixedSeqModel', 'MixedSeqLearner', 'mixed_seq_learner', 'load_all']

# Cell
import glob
import os
import pickle
import tempfile
import warnings
from pathlib import Path
from random import sample
from typing import List, Tuple

import numpy as np
import pandas as pd
from fastai.layers import trunc_normal_
from fastai.tabular.all import *
from fastai.text.all import *
from fastai.vision.all import *
from fastcore.basics import *
from fastcore.xtras import load_pickle, save_pickle
from progressbar import progressbar
from tsai.all import *

from ..datastruct.core import *
from ..datastruct.possessions import *

# Cell


@delegates()
class Embedding(nn.Embedding):
    """
    Embedding layer compatible with full pytorch API and truncated normal initialization

    Parameters
    ----------
    ni, nf: int
        input and output size of Embedding layer. It is the same
        as `num_embeddings` and `embedding_dim` in `torch.nn.Embedding()` module
    kwargs: dict
        Any argument accepted by `torch.nn.Embedding()` module
        a part from `num_embeddings` and `embedding_dim`
    std: float
        standard deviation applied in the truncated normal

    """

    def __init__(self, ni, nf, std=0.01, **kwargs):
        kwargs["num_embeddings"], kwargs["embedding_dim"] = ni, nf
        super().__init__(**kwargs)
        trunc_normal_(self.weight.data, std=std)


class MultiEmbedding(Module):
    """
    Muti-dimesnion Embedding layer

    Attributes
    ----------
    cat_embed: torch.nn.ModuleList
        list of Embedding modules in the order in which categorical data appear

    """

    def __init__(
        self,
        n_embeds: List[int],
        embed_dims: List[int] = None,
        embed_p: float = 0.0,
        n_cont: int = 0,
        std: float = 0.01,
        **kwargs
    ):
        """
        Initialise the various embedding sizes

        Parameters
        ----------
        n_embdes: List[int]
            length of the vocabulary of each categorical feature in the same order as passed in the tensor

        embed_dims: List[int]
            required size of each categorical feature embedding in the same order as passed in the tensor

        embed_p: float
            if non zero, applies a dropout layer to the the categorical features after embedding.

        n_cont: int, optional
            number of continuous features

        std: float
            standard deviation applied in the truncated normal
        kwargs: dict
            extra parameters passed to the embedding layer. Should be
            compatible with `torch.nn.Embedding()`

        """
        assert n_cont >= 0, "number of continuous features should be positive"
        self.n_cont = n_cont
        ## verify embedding size
        if embed_dims is None:
            embed_dims = [emb_sz_rule(s) for s in n_embeds]
        else:
            embed_dims = listify(embed_dims)
            if len(embed_dims) == 1:
                embed_dims = embed_dims * len(n_embeds)
            assert len(embed_dims) == len(n_embeds)

        self.emb_drop = nn.Dropout(embed_p)
        self.cat_embed = nn.ModuleList(
            [
                Embedding(ni=n, nf=d, std=std, **kwargs)
                for n, d in zip(n_embeds, embed_dims)
            ]
        )

    def forward(self, x_cat, x_cont=None):
        if isinstance(x_cat, tuple):
            if len(x_cat) == 2:
                x_cat, x_cont = x_cat
            elif len(x_cat) == 3:
                _, x_cat, x_cont = x_cat
            else:
                raise ValueError("x_cat is a tuple of unknown size")

        x_cat = torch.cat([e(x_cat[..., i]) for i, e in enumerate(self.cat_embed)], -1)
        x_cat = self.emb_drop(x_cat)
        if self.n_cont != 0:
            x_cat = torch.cat([x_cat, x_cont], -1)
        return x_cat

# Cell


@delegates(build_ts_model)
class MixedSeqModel(Module):
    "Sequence model with an embedding head."

    def __init__(
        self,
        arch: Module,
        n_cont: int,
        c_out: int,
        embded_config: dict = None,
        **kwargs
    ):
        """
        Intialise the model architecture

        Parameters
        ----------
        arch: Module
            one of tsai Model architectures accepted by `build_ts_model()`
        c_out: int
            number of output layers
        n_cont: int
            number of continuous features
        embed_config: dict
            all parameters accepted by the `MultiEmbedding` layer
        kwargs:
            Extra parameters accepted by `build_ts_model()`

        """
        ## head of the network
        embded_config["n_cont"] = n_cont
        self.head = MultiEmbedding(**embded_config)

        ## inialise the body
        self.arch, self.c_out, self.n_cont = arch, c_out, n_cont
        self.c_in = L(w.weight.shape[-1] for w in self.head.cat_embed).sum() + n_cont
        kwargs["arch"], kwargs["c_in"], kwargs["c_out"] = (
            self.arch,
            self.c_in,
            self.c_out,
        )

        self.body = build_ts_model(**kwargs)

    def forward(self, x_meta, x_cat, x_cont):
        if isinstance(x_meta, tuple):
            if len(x_meta) == 2:
                x_cat, x_cont = x_meta
            elif len(x_meta) == 3:
                _, x_cat, x_cont = x_meta
            else:
                raise ValueError("x_meta is a tuple of unknown size")
        x = self.head(x_cat, x_cont)

        return self.body(x.transpose(2, 1))

# Cell


class MixedSeqLearner(Learner):
    "`Learner` for mixed sequence data"

    def predict(self, files: List[Path]):
        "Predict a sequence of play read from a file"
        with self.no_bar():
            dl = self.dls.test_dl(listify(files))
            preds, _, cls_preds = self.get_preds(dl=dl, with_decoded=True)
            labels = dl.tfms.target_vocab[0]
            probs = pd.DataFrame(preds.detach().numpy(), columns=labels)
            clss = [labels[i] for i in cls_preds]

        return probs, clss


@delegates(build_ts_model)
def mixed_seq_learner(
    arch: Module,
    n_cont: int,
    c_out: int,
    embded_config: dict = None,
    # learner args
    dls=None,
    splitter=trainable_params,
    loss_func=None,
    opt_func=Adam,
    lr=defaults.lr,
    cbs=None,
    metrics=None,
    path=None,
    model_dir="models",
    wd=None,
    wd_bn_bias=False,
    train_bn=True,
    moms=(0.95, 0.85, 0.95),
    # other model args
    **kwargs
):
    """
    Interface to create a `Learner` for sequences with continuous and categorical features

    Parameters
    ----------
    arch: Module
        one of tsai Model architectures accepted by `build_ts_model()`
    c_out: int
        number of output layers
    n_cont: int
        number of continuous features
    embed_config: dict
        all parameters accepted by the `MultiEmbedding` layer

    Returns
    -------
    fastai.Learner
        Learner object with the `MixedSeqModel` model architecture

    """
    if arch is None:
        arch = LSTM

    model = MixedSeqModel(
        arch=arch, n_cont=n_cont, c_out=c_out, embded_config=embded_config, **kwargs
    )
    try:
        model.body[0], model.body[1]
        subscriptable = True
    except:
        subscriptable = False
    if subscriptable:
        splitter = ts_splitter
    if loss_func is None:
        if hasattr(dls, "loss_func"):
            loss_func = dls.loss_func
        elif hasattr(dls, "train_ds") and hasattr(dls.train_ds, "loss_func"):
            loss_func = dls.train_ds.loss_func
        elif hasattr(dls, "cat") and not dls.cat:
            loss_func = MSELossFlat()

    learn = MixedSeqLearner(
        dls=dls,
        model=model,
        loss_func=loss_func,
        opt_func=opt_func,
        lr=lr,
        cbs=cbs,
        metrics=metrics,
        path=path,
        splitter=splitter,
        model_dir=model_dir,
        wd=wd,
        wd_bn_bias=wd_bn_bias,
        train_bn=train_bn,
        moms=moms,
    )

    # keep track of args for loggers
    store_attr("arch", self=learn)

    return learn

# Cell


@patch
def predict_poss(
    self: MixedSeqLearner, seq_df: pd.DataFrame, verbose: bool = False
) -> pd.DataFrame:
    "Predict possession outcome probability in a sequentiel way"

    time_seconds = L(seq_df["time_seconds"].sort_values().tolist()).unique()

    def _get_probs(time):
        _df = seq_df[seq_df["time_seconds"] <= time]
        _dir = tempfile.TemporaryDirectory()
        _file = _df._id.values[0] + ".csv"

        file_name = Path(_dir.name) / _file
        res = None
        _df.to_csv(file_name)
        with self.no_bar():
            _proba = self.predict([Path(file_name)])[0]
        res = pd.DataFrame(
            {
                "time_seconds": time,
                "proba_goal": _proba["goal"],
                "proba_no-goal": 1.0 - _proba["goal"],
            },
            index=[0],
        )

        return res

    if verbose:
        _time_probs = [_get_probs(_time) for _time in progressbar(time_seconds)]
    else:
        _time_probs = [_get_probs(_time) for _time in time_seconds]
    time_probs = (
        pd.concat(_time_probs, axis=0)
        .sort_values(["time_seconds"])
        .reset_index(drop=True)
    )

    return seq_df.merge(time_probs, on="time_seconds", how="left")

# Cell


@patch
def predict_game(
    self: MixedSeqLearner,
    game_id: int,
    match_df: pd.DataFrame = None,
    save: bool = True,
) -> pd.DataFrame:
    "Predict (all the possessions in) a game"
    if match_df is None:
        match_df = Possession.get_all_game_poss(game_id)

    ## remove all existing documents
    if save:
        ActionValues.objects(game_id=game_id).delete()

    l_proba = []
    for poss_nbr in progressbar(match_df.possessionNumber.unique()):
        row = match_df[match_df.possessionNumber == poss_nbr]
        poss_proba = self.predict_poss(
            row.sequence.iloc[0].rename(columns={"possession_id": "_id"})
        ).rename(columns={"_id": "possession_id", "proba_no-goal": "proba_none"})
        poss_proba["game_id"] = row["gameId"].values[0]
        poss_proba["target"] = row["target"].values[0]

        ## add minutes and sec
        poss_proba["minutes"] = poss_proba.apply(
            lambda row: int((row.period_id - 1) * 45 + row.time_seconds // 60), axis=1
        )
        poss_proba["sec"] = poss_proba["time_seconds"].values % 60

        if save:
            ## create an actionValue object for each row
            lsave = L(ActionValues(**row.to_dict()) for _, row in poss_proba.iterrows())
            ActionValues.objects.insert(lsave)

        l_proba.append(poss_proba)

    probs_df = (
        pd.concat(l_proba)
        .reset_index(drop=True)
        .sort_values(["possession_number", "time_seconds"])
    )

    return probs_df

# Cell

@patch
def save_all(self:MixedSeqLearner, path='export', dls_fname='dls', model_fname='model', learner_fname='learner', do_save_dls=True, verbose=False):
    path = Path(path)
    if not os.path.exists(path): os.makedirs(path)

    if do_save_dls:
        self.dls_type = self.dls.__class__.__name__
        dls_fnames = []
        self.n_loaders = len(self.dls.loaders)
        for i,dl in enumerate(self.dls):
            dl = dl.new(num_workers=1)
            torch.save(dl, path/f'{dls_fname}_{i}.pth')
            dls_fnames.append(f'{dls_fname}_{i}.pth')

    # Saves the model along with optimizer
    self.model_dir = path
    self.save(f'{model_fname}', with_opt=True)

    # Export learn without the items and the optimizer state for inference
    self.export(path/f'{learner_fname}.pkl')

    pv(f'Learner saved:', verbose)
    pv(f"path          = '{path}'", verbose)
    if do_save_dls: pv(f"dls_fname     = '{dls_fnames}'", verbose)
    pv(f"model_fname   = '{model_fname}.pth'", verbose)
    pv(f"learner_fname = '{learner_fname}.pkl'", verbose)

def load_all(
    path="export",
    dls_fname="dls",
    model_fname="model",
    learner_fname="learner",
    device=None,
    pickle_module=pickle,
    do_load_dls=False,
    verbose=False,
):
    "Load a learner previously saved"
    if isinstance(device, int):
        device = torch.device("cuda", device)
    elif device is None:
        device = default_device()
    if device == "cpu":
        cpu = True
    else:
        cpu = None

    path = Path(path)
    learn = load_learner(
        path / f"{learner_fname}.pkl", cpu=cpu, pickle_module=pickle_module
    )
    learn.load(f"{model_fname}", with_opt=True, device=device)

    if do_load_dls:
        loaders = []
        dls_fnames = []
        for i in range(learn.n_loaders):
            dl = torch.load(
                path / f"{dls_fname}_{i}.pth",
                map_location=device,
                pickle_module=pickle_module,
            )
            dl = dl.new(num_workers=0)
            dl.to(device)
            first(dl)
            loaders.append(dl)
            dls_fnames.append(f"{dls_fname}_{i}.pth")
        learn.dls = type(learn.dls)(*loaders, path=learn.dls.path, device=device)

    pv(f"Learner loaded:", verbose)
    pv(f"path          = '{path}'", verbose)
    if do_load_dls: pv(f"dls_fname     = '{dls_fnames}'", verbose)
    pv(f"model_fname   = '{model_fname}.pth'", verbose)
    pv(f"learner_fname = '{learner_fname}.pkl'", verbose)

    return learn

# Cell

@typedispatch
def show_results(
    x: tuple,
    y,
    samples,
    outs,
    ctxs=None,
    max_n=6,
    nrows=None,
    ncols=1,
    figsize=None,
    **kwargs,
):
    n_elems = len(samples)
    if max_n > n_elems:
        max_n = n_elems
    if figsize is None:
        figsize = (10, 10 * 2.7)
    if ctxs is None:
        fig, ctxs = get_grid(
            max_n, nrows=None, ncols=ncols, figsize=figsize, return_fig=True
        )

    ## collect learner if available
    if "learner" in kwargs:
        learn = kwargs["learner"]
        labels = learn.dls.tfms.target_vocab[0]
        with learn.no_bar():
            probs, _, pred_cls = learn.get_preds(dl=[x], with_decoded=True)
        probs = pd.DataFrame(probs.detach().numpy(), columns=labels)
        for i, ctx in enumerate(ctxs):
            pred_class = labels[pred_cls[i].item()]
            proba = probs[pred_class].values[i]
            title = f"Actual: {samples[i][1]} \n Prediction: {pred_class} ({proba:.3f})"
            samples[i].show(ctx=ctx, fig=fig, title=title)
    else:
        for i, ctx in enumerate(ctxs):
            title = f'Actual: {samples[i][1]} \n Prediction: {["goal","no_goal"][y[i].item()]}'
            samples[i].show(ctx=ctx, fig=fig, title=title)